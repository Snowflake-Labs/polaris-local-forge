# Copyright 2025 Snowflake Inc.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

version: "3"

vars:
  # SKILL_DIR: where the polaris-local-forge source lives (follows symlinks)
  # This allows running tasks from isolated test folders
  SKILL_DIR:
    sh: dirname "$(readlink -f Taskfile.yml 2>/dev/null || echo Taskfile.yml)"
  # WORK_DIR: explicit work directory override (empty = use pwd)
  # Usage: task setup:all WORK_DIR=/path/to/project
  # Supports: ~/path, $HOME/path, $ENV_VAR/path
  WORK_DIR: ""
  # PROJECT_HOME: WORK_DIR if specified, otherwise current working directory
  # Uses eval to expand ~ and $ENV_VARS that Go-Task doesn't expand
  PROJECT_HOME:
    sh: |
      if [ -n "{{.WORK_DIR}}" ]; then
        eval echo "{{.WORK_DIR}}"
      else
        echo "$PWD"
      fi
  KUBECONFIG: "{{.PROJECT_HOME}}/.kube/config"
  K3D_CLUSTER_NAME:
    sh: basename $(pwd)
  K3S_VERSION: v1.35.1-k3s1
  FEATURES_DIR: "{{.PROJECT_HOME}}/k8s"
  PODMAN_MACHINE: k3d
  # PLF command - uses --project to find source, --work-dir for output
  PLF: uv run --project "{{.SKILL_DIR}}" polaris-local-forge --work-dir "{{.PROJECT_HOME}}"
  # Detect if running in source repo (has SKILL.md + src/polaris_local_forge)
  # Used to prevent accidental writes to the read-only source directory
  IS_SOURCE_REPO:
    sh: '[ -f "{{.PROJECT_HOME}}/SKILL.md" ] && [ -d "{{.PROJECT_HOME}}/src/polaris_local_forge" ] && echo "true" || echo "false"'
  # Files/directories to clean up on 'teardown -- all'
  # .snow-utils is ALWAYS preserved for replay/audit
  CLEANUP_PATHS: ".kube work k8s scripts bin notebooks .env .aws .envrc .gitignore .venv"

silent: true

env:
  KUBECONFIG: "{{.KUBECONFIG}}"
  # DOCKER_HOST via CLI - single source of truth for Podman SSH socket
  DOCKER_HOST:
    sh: uv run --project "{{.SKILL_DIR}}" polaris-local-forge --work-dir "{{.PROJECT_HOME}}" runtime docker-host 2>/dev/null || true

includes:
  podman: ./taskfiles/podman.yml
  cluster: ./taskfiles/cluster.yml
  polaris: ./taskfiles/polaris.yml
  catalog: ./taskfiles/catalog.yml
  ops: ./taskfiles/ops.yml
  l2c: ./taskfiles/l2c.yml

tasks:
  # ============================================
  # Installation
  # ============================================

  install:uv:
    desc: Install uv Python package manager
    cmds:
      - |
        if ! command -v uv &> /dev/null; then
          echo "Installing uv..."
          pip install uv || curl -LsSf https://astral.sh/uv/install.sh | sh
        else
          echo "uv is already installed ($(uv --version))"
        fi
    status:
      - command -v uv

  # ============================================
  # Manifest Management
  # ============================================

  manifest:init:
    desc: Initialize manifest with PENDING status
    summary: |
      Creates .snow-utils/snow-utils-manifest.md with all resources set to PENDING.
      
      Variables:
        WORK_DIR    Target directory (default: current directory)
      
      Example:
        task manifest:init
        task manifest:init WORK_DIR=~/polaris-dev
    cmds:
      - "{{.PLF}} setup manifest init"

  manifest:start:
    desc: Set manifest status to IN_PROGRESS
    summary: |
      Updates manifest status from PENDING to IN_PROGRESS.
      Call this at the start of the setup workflow.
      
      Example:
        task manifest:start
    cmds:
      - "{{.PLF}} setup manifest start"

  manifest:update:
    desc: Update a resource row to DONE
    summary: |
      Updates a specific resource row status to DONE.
      
      Arguments (pass after --):
        Resource number (1-7)
          1 = k3d cluster
          2 = RustFS
          3 = PostgreSQL
          4 = Polaris
          5 = Catalog
          6 = Principal
          7 = Demo data
      
      Example:
        task manifest:update -- 1   # Mark k3d cluster as DONE
        task manifest:update -- 4   # Mark Polaris as DONE
    cmds:
      - "{{.PLF}} setup manifest update {{.CLI_ARGS}}"

  manifest:complete:
    desc: Set manifest status to COMPLETE
    summary: |
      Updates manifest status to COMPLETE.
      Call this after all resources are successfully created.
      
      Example:
        task manifest:complete
    cmds:
      - "{{.PLF}} setup manifest complete"

  manifest:remove:
    desc: Set manifest status to REMOVED
    summary: |
      Updates manifest status and all resource statuses to REMOVED.
      Called automatically by teardown.
      
      Example:
        task manifest:remove
    cmds:
      - "{{.PLF}} setup manifest remove"

  # ============================================
  # Setup and Initialization
  # ============================================

  setup:python:
    desc: Set up Python environment using uv
    deps:
      - install:uv
    cmds:
      - uv python pin 3.12
      - uv venv
      - uv sync
      - |
        echo ""
        echo "Python environment setup complete."
        echo ""
        echo "Next step: Activate the virtual environment in your shell:"
        echo "  source .venv/bin/activate"
        echo ""

  prepare:
    desc: Generate required sensitive files from templates using Ansible
    summary: |
      Generates sensitive files from Ansible templates (RSA keys, K8s manifests, credentials).
      
      Variables:
        WORK_DIR    Target directory (default: current directory)
        TAGS        Comma-separated Ansible tags to run (default: all)
                    Available: rsa, setup, bootstrap, k8s
      
      Example:
        task prepare                     # Run all preparation steps
        task prepare TAGS=rsa,k8s        # Only RSA keys and K8s manifests
    preconditions:
      - sh: '[ "{{.IS_SOURCE_REPO}}" != "true" ]'
        msg: |
          ERROR: Cannot run prepare in the source repository.
          
          Use WORK_DIR to specify the target directory:
            task prepare WORK_DIR=~/polaris-dev
      - sh: test -f .venv/bin/activate || test -f .venv/Scripts/activate
        msg: "Python venv not found. Run 'task setup:python' first."
    cmds:
      - "{{.PLF}} prepare {{if .TAGS}}--tags={{.TAGS}}{{end}}"

  runtime:setup:
    desc: Ensure container runtime is ready (Podman machine setup if needed)
    cmds:
      - "{{.PLF}} setup runtime ensure"

  runtime:teardown:
    desc: Stop container runtime after teardown (Podman machine stop if needed)
    cmds:
      - "{{.PLF}} setup runtime stop"

  setup:all:
    desc: "Complete setup with manifest tracking. Use WORK_DIR=/path for isolated setup"
    summary: |
      Creates a complete Apache Polaris environment with manifest tracking.
      
      Variables:
        WORK_DIR    Isolated project directory (default: current directory)
                    Creates all files in specified directory instead of pwd
      
      Steps performed:
        1. Initialize manifest (PENDING)
        2. Run doctor checks and prepare configs
        3. Create k3d cluster, deploy RustFS + PostgreSQL
        4. Deploy Polaris, bootstrap principal
        5. Setup catalog with demo data
        6. Verify with DuckDB
      
      Manifest: .snow-utils/snow-utils-manifest.md
      
      Example:
        task setup:all                        # In current directory
        task setup:all WORK_DIR=~/polaris-dev # Isolated directory
    preconditions:
      - sh: '[ "{{.IS_SOURCE_REPO}}" != "true" ]'
        msg: |
          ERROR: Cannot run setup in the source repository.
          
          The skill directory is READ-ONLY. Use WORK_DIR to specify a separate directory:
            task setup:all WORK_DIR=~/polaris-dev
    cmds:
      # Initialize manifest with PENDING status
      - task: manifest:init
      # Set status to IN_PROGRESS
      - task: manifest:start
      # Same sequence as SKILL.md agentic workflow
      - "{{.PLF}} init"
      - "{{.PLF}} doctor --fix"
      - "{{.PLF}} prepare"
      - "{{.PLF}} cluster create"
      # Mark k3d cluster as DONE
      - task: manifest:update
        vars: { CLI_ARGS: "1" }
      - task: cluster:bootstrap-check
      # Mark RustFS and PostgreSQL as DONE (they come up with cluster)
      - task: manifest:update
        vars: { CLI_ARGS: "2" }
      - task: manifest:update
        vars: { CLI_ARGS: "3" }
      - "{{.PLF}} polaris deploy"
      - "{{.PLF}} polaris bootstrap"
      - task: cluster:polaris-check
      # Mark Polaris and Principal as DONE
      - task: manifest:update
        vars: { CLI_ARGS: "4" }
      - task: manifest:update
        vars: { CLI_ARGS: "6" }
      - "{{.PLF}} catalog setup"
      # Mark Catalog and Demo data as DONE
      - task: manifest:update
        vars: { CLI_ARGS: "5" }
      - task: manifest:update
        vars: { CLI_ARGS: "7" }
      - task: verify:sql
      # Set status to COMPLETE
      - task: manifest:complete
      - 'echo ""'
      - 'echo "Setup complete! Test data verified."'
      - 'echo "Manifest: .snow-utils/snow-utils-manifest.md"'
      - 'echo "Explore further with: task catalog:generate-notebook"'

  setup:replay:
    desc: "Replay or resume setup from manifest"
    summary: |
      Checks manifest status and continues appropriately:
      
      Status: REMOVED     → Full replay (re-run setup:all with existing config)
      Status: IN_PROGRESS → Resume from first PENDING resource
      Status: COMPLETE    → Already done, offer reset option
      No manifest         → Error, run setup:all instead
      
      Variables:
        WORK_DIR    Target directory containing the manifest
      
      Example:
        task setup:replay WORK_DIR=~/polaris-dev
    preconditions:
      - sh: '[ "{{.IS_SOURCE_REPO}}" != "true" ]'
        msg: |
          ERROR: Cannot run setup:replay in the source repository.
          
          Use WORK_DIR to specify the target directory:
            task setup:replay WORK_DIR=~/polaris-dev
    cmds:
      - |
        {{.PLF}} setup replay
        EXIT_CODE=$?
        case $EXIT_CODE in
          0)  ;; # COMPLETE and healthy - nothing to do
          10) task setup:all WORK_DIR="{{.PROJECT_HOME}}" ;;
          11) task setup:resume WORK_DIR="{{.PROJECT_HOME}}" ;;
          *)  exit $EXIT_CODE ;;
        esac

  setup:resume:
    desc: "Resume setup from IN_PROGRESS manifest (internal)"
    summary: |
      Resumes setup from an IN_PROGRESS manifest by checking which resources
      are still PENDING and running only those steps.
      
      This is typically called by setup:replay, not directly.
    cmds:
      - "{{.PLF}} setup resume"

  teardown:
    desc: "Teardown cluster with confirmation prompt"
    summary: |
      Deletes the k3d cluster and all Polaris resources.
      Always prompts for confirmation before proceeding.
      
      Options (pass after --):
        (none)      Cluster only - keeps all local files for replay
        all         Clean generated files, preserves .snow-utils for replay/audit
      
      Example:
        task teardown                  # Cluster only (keeps all local files)
        task teardown -- all           # Clean files, keep .snow-utils
    preconditions:
      - sh: '[ "{{.IS_SOURCE_REPO}}" != "true" ]'
        msg: |
          ERROR: Cannot run teardown in the source repository.
          
          Use WORK_DIR to specify the target directory:
            task teardown WORK_DIR=~/polaris-dev
    prompt: |
      This will delete the k3d cluster and all resources.
      
      Cleanup scope:
        - Default:      Cluster only (keeps all local files)
        - With 'all':   Clean generated files (preserves .snow-utils)
      
      Continue?
    cmds:
      - "{{.PLF}} teardown --yes"
      - task: manifest:remove
      - |
        if [ "{{.CLI_ARGS}}" = "all" ]; then
          echo "Cleaning local directory..."
          for item in {{.CLEANUP_PATHS}}; do
            rm -rf "{{.PROJECT_HOME}}/$item"
          done
          echo ""
          echo "Local directory cleaned."
          echo "Note: .snow-utils/ preserved for audit and replay."
        fi
      - 'echo ""'
      - 'echo "Teardown complete."'

  teardown:full:
    desc: Complete teardown - cluster + Podman stop + clean generated files
    summary: |
      Deletes the k3d cluster, all Polaris resources, stops Podman machine,
      and cleans generated files. Preserves .snow-utils for replay/audit.
      Always prompts for confirmation before proceeding.
      
      Example:
        task teardown:full WORK_DIR=~/polaris-dev
    preconditions:
      - sh: '[ "{{.IS_SOURCE_REPO}}" != "true" ]'
        msg: |
          ERROR: Cannot run teardown:full in the source repository.
          
          Use WORK_DIR to specify the target directory:
            task teardown:full WORK_DIR=~/polaris-dev
    prompt: |
      This will delete the k3d cluster, all resources, stop the Podman machine,
      AND clean generated files (preserves .snow-utils for replay/audit).
      
      Continue?
    cmds:
      - "{{.PLF}} teardown --yes --stop-podman"
      - task: manifest:remove
      - |
        echo "Cleaning local directory..."
        for item in {{.CLEANUP_PATHS}}; do
          rm -rf "{{.PROJECT_HOME}}/$item"
        done
        echo ""
        echo "Local directory cleaned."
        echo "Note: .snow-utils/ preserved for audit and replay."
      - 'echo ""'
      - 'echo "Teardown complete."'

  reset:all:
    desc: Complete reset - delete cluster, recreate everything with fresh catalog
    cmds:
      - task: teardown
      - task: setup:all

  clean:all:
    desc: Delete cluster and all resources (use teardown for full cleanup)
    cmds:
      - "{{.PLF}} teardown --yes --no-stop-podman"

  cluster:reset:
    desc: Delete and recreate cluster with fresh catalog
    cmds:
      - task: cluster:delete
      - task: cluster:create
      - task: cluster:bootstrap-check
      - task: polaris:deploy
      - task: cluster:polaris-check
      - task: catalog:setup
      - 'echo ""'
      - 'echo "Cluster reset complete."'
      - 'echo "All services are running with a fresh catalog."'

  catalog:reset:full:
    desc: Full reset - purge Polaris database and recreate catalog
    cmds:
      - echo "WARNING -- This will purge the entire Polaris database and recreate from scratch."
      - task: polaris:reset
      - task: catalog:setup
      - 'echo ""'
      - 'echo "Full catalog reset complete."'
      - 'echo "Polaris database was purged and catalog recreated."'
      - 'echo ""'
      - 'echo "Verification options:"'
      - 'echo "  - Run DuckDB verification: task verify"'
      - 'echo "  - Generate notebook: task catalog:generate-notebook"'

  # ============================================
  # Quick Reference / Aliases
  # ============================================

  doctor:
    desc: Check system prerequisites and health (uses CLI for consistent detection)
    summary: |
      Checks system prerequisites and attempts to fix issues.
      
      Options (pass after --):
        --fix       Attempt to fix issues automatically (e.g., start Podman machine)
        --verbose   Show detailed output
      
      Checks performed:
        - Required tools: podman/docker, k3d, kubectl, uv
        - Podman machine status (macOS only)
        - SSH config for Podman VM access (macOS only)
        - Port availability: 19000 (RustFS), 19001 (RustFS Console), 18181 (Polaris)
      
      Example:
        task doctor               # Check only
        task doctor -- --fix      # Check and fix
    cmds:
      - "{{.PLF}} doctor {{.CLI_ARGS}}"

  config:
    desc: Show current configuration from .env file
    summary: |
      Displays the current configuration values from .env file.
      
      Shows all non-commented, non-empty lines sorted alphabetically.
      
      Variables:
        WORK_DIR    Target directory (default: current directory)
      
      Example:
        task config
        task config WORK_DIR=~/polaris-dev
    cmds:
      - |
        if [ -f "{{.PROJECT_HOME}}/.env" ]; then
          echo "=== Configuration ({{.PROJECT_HOME}}/.env) ==="
          grep -v '^#' "{{.PROJECT_HOME}}/.env" | grep -v '^$' | sort
        else
          echo ".env file not found. Run 'task prepare' first."
        fi

  status:
    desc: Show status of cluster and Polaris
    summary: |
      Shows the current status of the k3d cluster and Polaris deployment.
      
      Displays:
        - k3d cluster list
        - Polaris namespace deployments and pods
      
      Example:
        task status
    cmds:
      - |
        echo "=== Cluster Status ==="
        k3d cluster list
        echo ""
        echo "=== Polaris Status ==="
        kubectl get deployments,pods -n polaris 2>/dev/null || echo "Cluster not running or polaris namespace not found"

  verify:
    desc: Verify Polaris catalog setup (alias for catalog:verify:duckdb)
    summary: |
      Verifies the Polaris catalog setup using DuckDB.
      
      Queries the demo table to confirm data is accessible through Polaris.
      
      Example:
        task verify
    cmds:
      - task: catalog:verify:duckdb
        vars:
          CLI_ARGS: "{{.CLI_ARGS}}"

  verify:sql:
    desc: Verify Polaris catalog setup using SQL (alias for catalog:verify:sql)
    summary: |
      Verifies the Polaris catalog setup using DuckDB CLI with SQL script.
      
      Runs a SQL script that queries the demo table through Polaris.
      
      Example:
        task verify:sql
    cmds:
      - task: catalog:verify:sql
        vars:
          CLI_ARGS: "{{.CLI_ARGS}}"


  # ============================================
  # Testing
  # ============================================

  test:isolated:
    desc: Create isolated test environment (protects source tree)
    cmds:
      - ./test/isolated-test.sh setup

  test:isolated:clean:
    desc: Remove all isolated test folders
    cmds:
      - ./test/isolated-test.sh clean

  test:isolated:list:
    desc: List existing isolated test folders
    cmds:
      - ./test/isolated-test.sh list

  help:
    desc: Show available tasks with descriptions
    summary: |
      Lists all available tasks with their descriptions.
      
      For detailed help on a specific task, use:
        task <task-name> --summary
      
      Common variables (pass as VAR=value):
        WORK_DIR    Isolated project directory (default: current dir)
        TAGS        Filter Ansible tags for prepare/catalog commands
      
      Example:
        task help                    # List all tasks
        task setup:all --summary     # Detailed help for setup:all
    cmds:
      - task --list
