# Copyright 2025 Snowflake Inc.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

version: "3"

vars:
  # SKILL_DIR: where the polaris-local-forge source lives (follows symlinks)
  # This allows running tasks from isolated test folders
  SKILL_DIR:
    sh: dirname "$(readlink -f Taskfile.yml 2>/dev/null || echo Taskfile.yml)"
  # PROJECT_HOME: current working directory (may be isolated test folder)
  PROJECT_HOME:
    sh: pwd
  KUBECONFIG: "{{.PROJECT_HOME}}/.kube/config"
  K3D_CLUSTER_NAME:
    sh: basename $(pwd)
  K3S_VERSION: v1.35.1-k3s1
  FEATURES_DIR: "{{.PROJECT_HOME}}/k8s"
  PODMAN_MACHINE: k3d
  # PLF command - uses --project to find source, --work-dir for output
  PLF: uv run --project "{{.SKILL_DIR}}" polaris-local-forge --work-dir "{{.PROJECT_HOME}}"

silent: true

env:
  KUBECONFIG: "{{.KUBECONFIG}}"
  # DOCKER_HOST via CLI - single source of truth for Podman SSH socket
  DOCKER_HOST:
    sh: uv run --project "{{.SKILL_DIR}}" polaris-local-forge --work-dir "{{.PROJECT_HOME}}" runtime docker-host 2>/dev/null || true

includes:
  podman: ./taskfiles/podman.yml
  cluster: ./taskfiles/cluster.yml
  polaris: ./taskfiles/polaris.yml
  catalog: ./taskfiles/catalog.yml
  ops: ./taskfiles/ops.yml

tasks:
  # ============================================
  # Installation
  # ============================================

  install:uv:
    desc: Install uv Python package manager
    cmds:
      - |
        if ! command -v uv &> /dev/null; then
          echo "Installing uv..."
          pip install uv || curl -LsSf https://astral.sh/uv/install.sh | sh
        else
          echo "uv is already installed ($(uv --version))"
        fi
    status:
      - command -v uv

  # ============================================
  # Setup and Initialization
  # ============================================

  setup:python:
    desc: Set up Python environment using uv
    deps:
      - install:uv
    cmds:
      - uv python pin 3.12
      - uv venv
      - uv sync
      - |
        echo ""
        echo "Python environment setup complete."
        echo ""
        echo "Next step: Activate the virtual environment in your shell:"
        echo "  source .venv/bin/activate"
        echo ""

  prepare:
    desc: Generate required sensitive files from templates using Ansible
    preconditions:
      - sh: test -f .venv/bin/activate || test -f .venv/Scripts/activate
        msg: "Python venv not found. Run 'task setup:python' first."
    cmds:
      - "{{.PLF}} prepare {{if .TAGS}}--tags={{.TAGS}}{{end}}"

  runtime:setup:
    desc: Ensure container runtime is ready (Podman machine setup if needed)
    cmds:
      - |
        RUNTIME="${PLF_CONTAINER_RUNTIME:-podman}"
        if [ "$RUNTIME" = "podman" ] && command -v podman > /dev/null 2>&1; then
          echo "Container runtime: podman"
          task podman:setup
        else
          echo "Container runtime: docker"
        fi

  runtime:teardown:
    desc: Stop container runtime after teardown (Podman machine stop if needed)
    cmds:
      - |
        RUNTIME="${PLF_CONTAINER_RUNTIME:-podman}"
        if [ "$RUNTIME" = "podman" ] && [ "$(uname)" = "Darwin" ] && command -v podman > /dev/null 2>&1; then
          task podman:stop
        fi

  setup:all:
    desc: Complete setup - init, doctor, cluster, Polaris, catalog (mirrors agentic workflow)
    cmds:
      # Same sequence as SKILL.md agentic workflow - one base, multiple use
      - "{{.PLF}} init"
      - "{{.PLF}} doctor --fix"
      - "{{.PLF}} prepare"
      - "{{.PLF}} cluster create"
      - "{{.PLF}} polaris deploy"
      - task: cluster:bootstrap-check
      - "{{.PLF}} polaris bootstrap"
      - task: cluster:polaris-check
      - "{{.PLF}} catalog setup"
      - task: verify:sql
      - 'echo ""'
      - 'echo "Setup complete! Test data verified."'
      - 'echo "Explore further with: task catalog:generate-notebook"'

  teardown:
    desc: Complete teardown - cleanup catalog, delete cluster, prompt to stop Podman
    cmds:
      - "{{.PLF}} teardown --yes"

  teardown:full:
    desc: Complete teardown with Podman stop (no prompt)
    cmds:
      - "{{.PLF}} teardown --yes --stop-podman"

  reset:all:
    desc: Complete reset - delete cluster, recreate everything with fresh catalog
    cmds:
      - task: teardown
      - task: setup:all

  clean:all:
    desc: Delete cluster and all resources (use teardown for full cleanup)
    cmds:
      - "{{.PLF}} teardown --yes --no-stop-podman"

  cluster:reset:
    desc: Delete and recreate cluster with fresh catalog
    cmds:
      - task: cluster:delete
      - task: cluster:create
      - task: cluster:bootstrap-check
      - task: polaris:deploy
      - task: cluster:polaris-check
      - task: catalog:setup
      - 'echo ""'
      - 'echo "Cluster reset complete."'
      - 'echo "All services are running with a fresh catalog."'

  catalog:reset:full:
    desc: Full reset - purge Polaris database and recreate catalog
    cmds:
      - echo "WARNING -- This will purge the entire Polaris database and recreate from scratch."
      - task: polaris:reset
      - task: catalog:setup
      - 'echo ""'
      - 'echo "Full catalog reset complete."'
      - 'echo "Polaris database was purged and catalog recreated."'
      - 'echo ""'
      - 'echo "Verification options:"'
      - 'echo "  - Run DuckDB verification: task verify"'
      - 'echo "  - Generate notebook: task catalog:generate-notebook"'

  # ============================================
  # Quick Reference / Aliases
  # ============================================

  doctor:
    desc: Check system prerequisites and health (uses CLI for consistent detection)
    cmds:
      - "{{.PLF}} doctor {{.CLI_ARGS}}"

  config:
    desc: Show current configuration from .env file
    cmds:
      - |
        if [ -f .env ]; then
          echo "=== Configuration (.env) ==="
          grep -v '^#' .env | grep -v '^$' | sort
        else
          echo ".env file not found. Run 'task prepare' first."
        fi

  status:
    desc: Show status of cluster and Polaris
    cmds:
      - |
        echo "=== Cluster Status ==="
        k3d cluster list
        echo ""
        echo "=== Polaris Status ==="
        kubectl get deployments,pods -n polaris 2>/dev/null || echo "Cluster not running or polaris namespace not found"

  verify:
    desc: Verify Polaris catalog setup (alias for catalog:verify:duckdb)
    cmds:
      - task: catalog:verify:duckdb
        vars:
          CLI_ARGS: "{{.CLI_ARGS}}"

  verify:sql:
    desc: Verify Polaris catalog setup using SQL (alias for catalog:verify:sql)
    cmds:
      - task: catalog:verify:sql
        vars:
          CLI_ARGS: "{{.CLI_ARGS}}"


  # ============================================
  # Testing
  # ============================================

  test:isolated:
    desc: Create isolated test environment (protects source tree)
    cmds:
      - ./test/isolated-test.sh setup

  test:isolated:clean:
    desc: Remove all isolated test folders
    cmds:
      - ./test/isolated-test.sh clean

  test:isolated:list:
    desc: List existing isolated test folders
    cmds:
      - ./test/isolated-test.sh list

  help:
    desc: Show available tasks with descriptions
    cmds:
      - task --list
