# Copyright 2025 Snowflake Inc.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""CLI entry point for Polaris Local Forge.

Thin wrapper following kamesh-demo-skills pattern:
- CLI reads .env via load_dotenv(), never writes
- Each command does ONE thing: validate → run subprocess → exit
- SKILL.md handles orchestration, manifest updates, .env updates
"""

import json
import os
import platform
import re
import shutil
import subprocess
import sys
from pathlib import Path

import click
import yaml
from dotenv import load_dotenv

# Skill directory: where the source repo lives
SKILL_DIR = Path(__file__).parent.parent.parent.resolve()
ANSIBLE_DIR = SKILL_DIR / "polaris-forge-setup"
ANSIBLE_DEFAULTS_FILE = ANSIBLE_DIR / "defaults" / "main.yml"

# Mapping from Ansible variable names to CLI env var names
_ANSIBLE_TO_ENV = {
    "plf_cluster_name": "K3D_CLUSTER_NAME",
    "plf_k3s_version": "K3S_VERSION",
    "plf_container_runtime": "PLF_CONTAINER_RUNTIME",
    "plf_podman_machine": "PLF_PODMAN_MACHINE",
    "plf_realm": "POLARIS_REALM",
    "plf_admin_username": "PLF_POLARIS_PRINCIPAL_NAME",
}


def _load_ansible_defaults() -> dict:
    """Load defaults from Ansible defaults/main.yml."""
    defaults = {}
    if ANSIBLE_DEFAULTS_FILE.exists():
        with open(ANSIBLE_DEFAULTS_FILE) as f:
            ansible_vars = yaml.safe_load(f) or {}
        for ansible_key, env_key in _ANSIBLE_TO_ENV.items():
            if ansible_key in ansible_vars:
                defaults[env_key] = ansible_vars[ansible_key]
    return defaults


DEFAULTS = _load_ansible_defaults()

# Static k8s files to copy when work_dir differs from SKILL_DIR
# NOTE: postgresql.yaml and polaris.yaml are GENERATED by prepare.yml, not static
STATIC_K8S_FILES = [
    "k8s/features/rustfs.yaml",
    "k8s/polaris/kustomization.yaml",
    "k8s/polaris/jobs/kustomization.yaml",
    "k8s/polaris/jobs/job-bootstrap.yaml",
    "k8s/polaris/jobs/job-purge.yaml",
]

# Template files to copy during init: (source_rel_path, dest_filename, chmod_mode or None)
TEMPLATES = [
    (".env.example", ".env", 0o600),
    ("user-project/.envrc.example", ".envrc", None),
    ("user-project/.gitignore.example", ".gitignore", None),
]

# Directories to create during init
INIT_DIRECTORIES = [".kube", "work", "bin", "k8s/features", "k8s/polaris/jobs"]

# Manifest template for .snow-utils/snow-utils-manifest.md
MANIFEST_TEMPLATE = """# Snow-Utils Manifest

This manifest tracks resources created by polaris-local-forge.

---

**Status:** PENDING

## project_recipe
project_name: {project_name}

## configuration
container_runtime: {container_runtime}
podman_machine: {podman_machine}
cluster_name: {cluster_name}

## resources

| # | Resource | Type | Status |
|---|----------|------|--------|
| 1 | k3d cluster | infrastructure | PENDING |
| 2 | RustFS | storage | PENDING |
| 3 | PostgreSQL | database | PENDING |
| 4 | Polaris | service | PENDING |
| 5 | Catalog | data | PENDING |
| 6 | Principal | auth | PENDING |
| 7 | Demo data | data | PENDING |

## prereqs

## installed_skills
polaris-local-forge: https://github.com/kameshsampath/polaris-local-forge
"""


def get_config(work_dir: Path) -> dict:
    """Load config from .env file."""
    env_file = work_dir / ".env"
    if env_file.exists():
        load_dotenv(env_file, override=True)
    return {k: os.getenv(k, default) for k, default in DEFAULTS.items()}


def get_podman_ssh_uri(machine_name: str) -> str | None:
    """Get SSH URI for a Podman machine (macOS only).

    On macOS, k3d must use an SSH-based DOCKER_HOST (not a local Unix socket)
    to avoid volume-mount failures inside the Podman VM.
    """
    try:
        result = subprocess.run(
            ["podman", "system", "connection", "ls", "--format", "json"],
            capture_output=True, text=True, check=True
        )
        connections = json.loads(result.stdout)
        # Look for the machine's root connection (e.g., "k3d-root")
        for conn in connections:
            if conn.get("Name") == f"{machine_name}-root":
                return conn.get("URI")  # e.g., ssh://root@127.0.0.1:PORT/run/podman/podman.sock
        # Fallback: try exact machine name
        for conn in connections:
            if conn.get("Name") == machine_name:
                return conn.get("URI")
    except (subprocess.CalledProcessError, json.JSONDecodeError, FileNotFoundError):
        pass
    return None


def get_runtime_env(cfg: dict) -> dict:
    """Get environment variables for container runtime."""
    env = os.environ.copy()
    runtime = cfg.get("PLF_CONTAINER_RUNTIME", "podman")
    machine = cfg.get("PLF_PODMAN_MACHINE", "k3d")

    if runtime == "podman" and platform.system() == "Darwin":
        # macOS: use SSH-based DOCKER_HOST for Podman machine
        ssh_uri = get_podman_ssh_uri(machine)
        if ssh_uri:
            env["DOCKER_HOST"] = ssh_uri
    return env


def run_ansible(playbook: str, work_dir: Path, tags: str | None = None,
                dry_run: bool = False, verbose: bool = False,
                require_aws: bool = True) -> int:
    """Run ansible playbook, return exit code."""
    playbook_path = ANSIBLE_DIR / playbook
    if not playbook_path.exists():
        click.echo(f"Playbook not found: {playbook_path}", err=True)
        return 1

    cmd = ["uv", "run", "ansible-playbook", str(playbook_path)]

    # CRITICAL: Override plf_output_base to write to work_dir instead of skill_dir
    # Without this, Ansible uses playbook_dir/.. which is the skill source directory
    cmd.extend(["-e", f"plf_output_base={work_dir}"])

    # CRITICAL: Use the current Python interpreter (has boto3/botocore)
    # Without this, Ansible discovers system Python which lacks dependencies
    cmd.extend(["-e", f"ansible_python_interpreter={sys.executable}"])

    if tags:
        cmd.extend(["--tags", tags])
    if verbose:
        cmd.append("-v")

    # Set ANSIBLE_CONFIG explicitly
    env = os.environ.copy()
    env["ANSIBLE_CONFIG"] = str(ANSIBLE_DIR / "ansible.cfg")

    # Isolate from user's AWS config - use project-local RustFS config instead
    # Remove profile/session env vars that could conflict with local S3
    for var in ["AWS_PROFILE", "AWS_DEFAULT_PROFILE", "AWS_SESSION_TOKEN",
                "AWS_REGION", "AWS_DEFAULT_REGION", "AWS_ACCESS_KEY_ID",
                "AWS_SECRET_ACCESS_KEY"]:
        env.pop(var, None)
    # Point to project-local .aws/ files (generated by 'prepare' command)
    # This prevents boto3 from reading ~/.aws/ (SSO tokens, profiles, etc.)
    project_aws_config = work_dir / ".aws" / "config"
    project_aws_creds = work_dir / ".aws" / "credentials"
    if require_aws:
        if not project_aws_config.exists():
            click.echo(f"Error: {project_aws_config} not found. Run 'plf prepare' first.", err=True)
            return 1
        env["AWS_CONFIG_FILE"] = str(project_aws_config)
        env["AWS_SHARED_CREDENTIALS_FILE"] = str(project_aws_creds)
    else:
        # For prepare.yml which creates .aws/ - block host config but don't require local
        env["AWS_CONFIG_FILE"] = "/dev/null"
        env["AWS_SHARED_CREDENTIALS_FILE"] = "/dev/null"

    # Set KUBECONFIG so kubernetes.core can find the cluster
    kubeconfig = work_dir / ".kube" / "config"
    if kubeconfig.exists():
        env["KUBECONFIG"] = str(kubeconfig)

    if dry_run:
        click.echo(f"Would run: {' '.join(cmd)}")
        return 0

    result = subprocess.run(cmd, cwd=work_dir, env=env)
    return result.returncode


def copy_static_files(work_dir: Path) -> None:
    """Copy static k8s files from skill dir to work dir."""
    if work_dir.resolve() == SKILL_DIR.resolve():
        return
    for rel_path in STATIC_K8S_FILES:
        src = SKILL_DIR / rel_path
        dst = work_dir / rel_path
        if src.exists():
            dst.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dst)


# =============================================================================
# CLI Entry Point
# =============================================================================

@click.group()
@click.option("--work-dir", "-w", type=click.Path(exists=True, file_okay=False),
              help="Project directory (defaults to current directory)")
@click.pass_context
def cli(ctx, work_dir: str | None):
    """Polaris Local Forge - Local Iceberg development environment."""
    ctx.ensure_object(dict)
    ctx.obj["WORK_DIR"] = Path(work_dir).resolve() if work_dir else Path.cwd().resolve()
    ctx.obj["K8S_DIR"] = ctx.obj["WORK_DIR"] / "k8s"
    ctx.obj["CONFIG"] = get_config(ctx.obj["WORK_DIR"])


# =============================================================================
# Init Command
# =============================================================================

@cli.command("init")
@click.option("--force", "-f", is_flag=True, help="Overwrite existing files")
@click.option("--cluster-name", "-n", help="Cluster name (defaults to directory name)")
@click.option("--with-manifest", "-m", is_flag=True, help="Initialize .snow-utils manifest")
@click.pass_context
def init_project(ctx, force: bool, cluster_name: str | None, with_manifest: bool):
    """Initialize project directory with .env and configuration files."""
    work_dir = ctx.obj["WORK_DIR"]
    project_name = cluster_name or work_dir.name
    created = []
    skipped = []

    # Copy template files
    for src_rel, dst_name, mode in TEMPLATES:
        src = SKILL_DIR / src_rel
        dst = work_dir / dst_name
        if src.exists():
            if not dst.exists() or force:
                shutil.copy2(src, dst)
                if mode:
                    dst.chmod(mode)
                created.append(dst_name)
            else:
                skipped.append(dst_name)

    # Create directories
    for d in INIT_DIRECTORIES:
        dir_path = work_dir / d
        if not dir_path.exists():
            dir_path.mkdir(parents=True, exist_ok=True)
            created.append(f"{d}/")

    # Set PROJECT_HOME, K3D_CLUSTER_NAME, and SKILL_DIR in .env
    env_file = work_dir / ".env"
    env_vars_added = []
    if env_file.exists():
        env_content = env_file.read_text()
        additions = []
        # Use regex to match uncommented lines (not starting with #)
        if not re.search(r'^PROJECT_HOME=', env_content, re.MULTILINE):
            additions.append(f"PROJECT_HOME={work_dir}")
            env_vars_added.append(f"PROJECT_HOME={work_dir}")
        if not re.search(r'^K3D_CLUSTER_NAME=', env_content, re.MULTILINE):
            additions.append(f"K3D_CLUSTER_NAME={project_name}")
            env_vars_added.append(f"K3D_CLUSTER_NAME={project_name}")
        if not re.search(r'^SKILL_DIR=', env_content, re.MULTILINE):
            additions.append(f"SKILL_DIR={SKILL_DIR}")
            env_vars_added.append(f"SKILL_DIR={SKILL_DIR}")
        if additions:
            with open(env_file, "a") as f:
                f.write("\n" + "\n".join(additions) + "\n")

    # Initialize manifest if requested
    if with_manifest:
        manifest_dir = work_dir / ".snow-utils"
        manifest_file = manifest_dir / "snow-utils-manifest.md"
        if not manifest_file.exists() or force:
            manifest_dir.mkdir(mode=0o700, parents=True, exist_ok=True)
            # Get configuration values from .env or defaults
            cfg = ctx.obj.get("CONFIG", {})
            container_runtime = cfg.get("PLF_CONTAINER_RUNTIME", "podman")
            podman_machine = cfg.get("PLF_PODMAN_MACHINE", "k3d")
            manifest_file.write_text(MANIFEST_TEMPLATE.format(
                project_name=project_name,
                container_runtime=container_runtime,
                podman_machine=podman_machine if container_runtime == "podman" else "N/A",
                cluster_name=project_name
            ))
            manifest_file.chmod(0o600)
            created.append(".snow-utils/snow-utils-manifest.md")

    # Create bin/plf wrapper script
    bin_dir = work_dir / "bin"
    plf_script = bin_dir / "plf"
    if not plf_script.exists() or force:
        bin_dir.mkdir(parents=True, exist_ok=True)
        plf_script.write_text(f'''#!/bin/bash
# Polaris Local Forge wrapper - auto-generated by init
source "$(dirname "$0")/../.env" 2>/dev/null
exec uv run --quiet --project "$SKILL_DIR" polaris-local-forge --work-dir "$PROJECT_HOME" "$@"
''')
        plf_script.chmod(0o755)
        created.append("bin/plf")

    # Output summary
    if created:
        click.echo(f"Created: {', '.join(created)}")
    if env_vars_added:
        for var in env_vars_added:
            click.echo(f"Set: {var}")
    if skipped:
        click.echo(f"Skipped (exists): {', '.join(skipped)}")
    if not created and not skipped and not env_vars_added:
        click.echo("Nothing to do.")


# =============================================================================
# Doctor Command
# =============================================================================

def check_tool(name: str) -> bool:
    """Check if a tool is available in PATH."""
    return shutil.which(name) is not None


def check_port(port: int) -> tuple[bool, str | None]:
    """Check if a port is available. Returns (available, process_name)."""
    try:
        result = subprocess.run(
            ["lsof", "-i", f":{port}", "-t"],
            capture_output=True, text=True
        )
        if result.returncode == 0 and result.stdout.strip():
            pid = result.stdout.strip().split("\n")[0]
            ps_result = subprocess.run(
                ["ps", "-p", pid, "-o", "comm="],
                capture_output=True, text=True
            )
            proc_name = ps_result.stdout.strip() if ps_result.returncode == 0 else "unknown"
            return False, proc_name
        return True, None
    except Exception:
        return True, None


def get_podman_machine_state(machine_name: str) -> str | None:
    """Get Podman machine state (running, stopped, etc.)."""
    try:
        result = subprocess.run(
            ["podman", "machine", "inspect", machine_name, "--format", "{{.State}}"],
            capture_output=True, text=True
        )
        if result.returncode == 0:
            return result.stdout.strip().lower()
    except Exception:
        pass
    return None


def check_runtime_available(cfg: dict) -> bool:
    """Check if container runtime (Podman/Docker) is available."""
    runtime = cfg.get("PLF_CONTAINER_RUNTIME", "podman")
    machine = cfg.get("PLF_PODMAN_MACHINE", "k3d")

    if platform.system() == "Darwin" and runtime == "podman":
        state = get_podman_machine_state(machine)
        return state == "running"

    # For Docker or Linux Podman, check if daemon is accessible
    try:
        subprocess.run([runtime, "info"], capture_output=True, check=True)
        return True
    except Exception:
        return False


def get_podman_identity(machine_name: str) -> str | None:
    """Get the identity file path for a Podman machine."""
    try:
        result = subprocess.run(
            ["podman", "system", "connection", "ls", "--format", "json"],
            capture_output=True, text=True, check=True
        )
        connections = json.loads(result.stdout)
        for conn in connections:
            if conn.get("Name") == f"{machine_name}-root":
                return conn.get("Identity")
    except Exception:
        pass
    return None


def setup_ssh_config(machine_name: str) -> bool:
    """Setup SSH config for Podman machine (macOS only)."""
    identity = get_podman_identity(machine_name)
    if not identity or not Path(identity).exists():
        return False

    # Add SSH key to agent
    subprocess.run(["ssh-add", identity], capture_output=True)

    # Update ~/.ssh/config
    ssh_dir = Path.home() / ".ssh"
    ssh_dir.mkdir(mode=0o700, exist_ok=True)
    ssh_config = ssh_dir / "config"

    marker = "# polaris-local-forge podman machine"
    if ssh_config.exists() and marker in ssh_config.read_text():
        return True  # Already configured

    config_entry = f"""
{marker}
Host 127.0.0.1
    IdentityFile {identity}
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
"""
    with open(ssh_config, "a") as f:
        f.write(config_entry)
    ssh_config.chmod(0o600)
    return True


def kill_gvproxy() -> bool:
    """Kill gvproxy processes that may be holding ports."""
    try:
        result = subprocess.run(
            ["pkill", "-9", "gvproxy"],
            capture_output=True
        )
        return result.returncode == 0
    except Exception:
        return False


@cli.command("doctor")
@click.option("--fix", is_flag=True, help="Attempt to fix issues automatically")
@click.option("--output", type=click.Choice(["text", "json"]), default="text",
              help="Output format")
@click.pass_context
def doctor(ctx, fix: bool, output: str):
    """Check prerequisites and environment status."""
    work_dir = ctx.obj["WORK_DIR"]
    # Auto-run init if .env doesn't exist
    if not (work_dir / ".env").exists():
        click.echo("Project not initialized. Running 'init' first...")
        ctx.invoke(init_project)
        # Reload config after init
        ctx.obj["CONFIG"] = get_config(work_dir)
    cfg = ctx.obj["CONFIG"]
    runtime = cfg.get("PLF_CONTAINER_RUNTIME", "podman")
    machine = cfg.get("PLF_PODMAN_MACHINE", "k3d")
    is_macos = platform.system() == "Darwin"

    issues = []
    checks = []

    # Check required tools
    required_tools = ["k3d", "kubectl", "uv"]
    if runtime == "podman":
        required_tools.insert(0, "podman")
    else:
        required_tools.insert(0, "docker")

    for tool in required_tools:
        ok = check_tool(tool)
        checks.append({"name": f"tool:{tool}", "ok": ok})
        if not ok:
            issues.append(f"Tool '{tool}' not found in PATH")

    # Check Podman machine (macOS only)
    if runtime == "podman" and is_macos:
        state = get_podman_machine_state(machine)
        if state is None:
            checks.append({"name": f"podman-machine:{machine}", "ok": False, "state": "not found"})
            issues.append(f"Podman machine '{machine}' not found. Run: task podman:setup:machine")
        elif state != "running":
            checks.append({"name": f"podman-machine:{machine}", "ok": False, "state": state})
            if fix:
                click.echo(f"Starting Podman machine '{machine}'...")
                result = subprocess.run(["podman", "machine", "start", machine], capture_output=True)
                if result.returncode == 0:
                    click.echo(f"Started Podman machine '{machine}'")
                    checks[-1]["ok"] = True
                    checks[-1]["state"] = "running"
                else:
                    issues.append(f"Failed to start Podman machine '{machine}'")
            else:
                issues.append(f"Podman machine '{machine}' is {state}. Run with --fix or: podman machine start {machine}")
        else:
            checks.append({"name": f"podman-machine:{machine}", "ok": True, "state": "running"})

        # Check SSH config
        ssh_config = Path.home() / ".ssh" / "config"
        marker = "# polaris-local-forge podman machine"
        ssh_configured = ssh_config.exists() and marker in ssh_config.read_text()
        checks.append({"name": "ssh-config", "ok": ssh_configured})
        if not ssh_configured:
            if fix:
                click.echo("Setting up SSH config for Podman VM...")
                if setup_ssh_config(machine):
                    click.echo("SSH config updated")
                    checks[-1]["ok"] = True
                else:
                    issues.append("Failed to setup SSH config")
            else:
                issues.append("SSH config not configured. Run with --fix or: task podman:setup:machine")

    # Check ports (k3d handles 6443 internally)
    ports = [(19000, "RustFS S3 API"), (19001, "RustFS Console"), (18181, "Polaris API")]
    for port, desc in ports:
        available, proc = check_port(port)
        checks.append({"name": f"port:{port}", "ok": available, "desc": desc, "blocker": proc})
        if not available:
            msg = f"Port {port} ({desc}) in use by {proc}"
            if proc == "gvproxy" and fix:
                click.echo(f"Killing gvproxy holding port {port}...")
                kill_gvproxy()
                # Recheck
                available, _ = check_port(port)
                if available:
                    checks[-1]["ok"] = True
                    click.echo(f"Port {port} now available")
                else:
                    issues.append(msg)
            else:
                issues.append(msg)

    # Output results
    if output == "json":
        result = {"checks": checks, "issues": issues, "ok": len(issues) == 0}
        click.echo(json.dumps(result, indent=2))
    else:
        click.echo("Polaris Local Forge - Environment Check")
        click.echo("=" * 40)
        for check in checks:
            status = "OK" if check["ok"] else "FAIL"
            name = check["name"]
            extra = ""
            if "state" in check:
                extra = f" ({check['state']})"
            elif "blocker" in check and check["blocker"]:
                extra = f" (blocked by {check['blocker']})"
            click.echo(f"  [{status}] {name}{extra}")

        if issues:
            click.echo("")
            click.echo("Issues:")
            for issue in issues:
                click.echo(f"  - {issue}")
            sys.exit(1)
        else:
            click.echo("")
            click.echo("All checks passed!")


# =============================================================================
# Prepare Command
# =============================================================================

@cli.command()
@click.option("--tags", "-t", help="Ansible tags (comma-separated)")
@click.option("--dry-run", "-n", is_flag=True, help="Preview without executing")
@click.option("--verbose", "-v", is_flag=True, help="Verbose output")
@click.pass_context
def prepare(ctx, tags: str | None, dry_run: bool, verbose: bool):
    """Generate configuration files from templates."""
    work_dir = ctx.obj["WORK_DIR"]
    # Auto-run init if .env doesn't exist
    if not (work_dir / ".env").exists():
        click.echo("Project not initialized. Running 'init' first...")
        ctx.invoke(init_project)
        ctx.obj["CONFIG"] = get_config(work_dir)
    # require_aws=False because prepare.yml CREATES .aws/config
    exit_code = run_ansible("prepare.yml", work_dir, tags=tags, dry_run=dry_run,
                            verbose=verbose, require_aws=False)
    if exit_code == 0 and not dry_run:
        copy_static_files(work_dir)
    sys.exit(exit_code)


# =============================================================================
# Cluster Commands
# =============================================================================

@cli.group()
def cluster():
    """Kubernetes cluster management."""
    pass


@cluster.command("create")
@click.option("--dry-run", "-n", is_flag=True, help="Preview without executing")
@click.pass_context
def cluster_create(ctx, dry_run: bool):
    """Create k3d cluster using config/cluster-config.yaml."""
    work_dir = ctx.obj["WORK_DIR"]
    # Auto-run init if .env doesn't exist
    if not (work_dir / ".env").exists():
        click.echo("Project not initialized. Running 'init' first...")
        ctx.invoke(init_project)
        ctx.obj["CONFIG"] = get_config(work_dir)
    cfg = ctx.obj["CONFIG"]
    k8s_dir = ctx.obj["K8S_DIR"]
    config_file = SKILL_DIR / "config" / "cluster-config.yaml"

    env = get_runtime_env(cfg)
    env["KUBECONFIG"] = str(work_dir / ".kube" / "config")
    # Env vars for cluster-config.yaml substitution
    env["K3D_CLUSTER_NAME"] = cfg["K3D_CLUSTER_NAME"]
    env["K3S_VERSION"] = cfg["K3S_VERSION"]
    env["FEATURES_DIR"] = str(k8s_dir)

    cmd = ["k3d", "cluster", "create", "--config", str(config_file)]

    if dry_run:
        click.echo(f"Would run: {' '.join(cmd)}")
        click.echo(f"  K3D_CLUSTER_NAME={env['K3D_CLUSTER_NAME']}")
        click.echo(f"  K3S_VERSION={env['K3S_VERSION']}")
        click.echo(f"  FEATURES_DIR={env['FEATURES_DIR']}")
        return

    result = subprocess.run(cmd, env=env)
    sys.exit(result.returncode)


@cluster.command("delete")
@click.option("--dry-run", "-n", is_flag=True, help="Preview without executing")
@click.option("--yes", "-y", is_flag=True, help="Skip confirmation")
@click.pass_context
def cluster_delete(ctx, dry_run: bool, yes: bool):
    """Delete k3d cluster."""
    cfg = ctx.obj["CONFIG"]
    cluster_name = cfg["K3D_CLUSTER_NAME"]
    env = get_runtime_env(cfg)

    if not yes and not dry_run:
        if not click.confirm(f"Delete cluster '{cluster_name}'?"):
            click.echo("Aborted.")
            return

    cmd = ["k3d", "cluster", "delete", cluster_name]

    if dry_run:
        click.echo(f"Would run: {' '.join(cmd)}")
        return

    result = subprocess.run(cmd, env=env)
    sys.exit(result.returncode)


@cluster.command("wait")
@click.option("--tags", "-t", help="Ansible tags: bootstrap (rustfs+pg), polaris")
@click.option("--dry-run", "-n", is_flag=True, help="Preview without executing")
@click.option("--verbose", "-v", is_flag=True, help="Verbose output")
@click.pass_context
def cluster_wait(ctx, tags: str | None, dry_run: bool, verbose: bool):
    """Wait for cluster resources to be ready (wraps cluster_checks.yml)."""
    cfg = ctx.obj["CONFIG"]
    work_dir = ctx.obj["WORK_DIR"]

    if not check_runtime_available(cfg):
        click.echo("Container runtime not running. Run: plf doctor --fix")
        sys.exit(1)

    # require_aws=False because cluster_checks.yml only uses kubernetes.core
    exit_code = run_ansible("cluster_checks.yml", work_dir, tags=tags,
                           dry_run=dry_run, verbose=verbose, require_aws=False)
    sys.exit(exit_code)


@cluster.command("list")
@click.option("--output", "-o", type=click.Choice(["text", "json"]), default="text",
              help="Output format")
@click.pass_context
def cluster_list(ctx, output: str):
    """List k3d clusters."""
    cfg = ctx.obj["CONFIG"]
    env = get_runtime_env(cfg)

    if not check_runtime_available(cfg):
        if output == "json":
            click.echo(json.dumps({"error": "runtime_not_running", "clusters": []}))
        else:
            click.echo("Container runtime not running. Run: plf doctor --fix")
        sys.exit(1)

    result = subprocess.run(
        ["k3d", "cluster", "list", "-o", "json"],
        env=env, capture_output=True, text=True
    )

    if result.returncode != 0:
        click.echo(result.stderr, err=True)
        sys.exit(result.returncode)

    if output == "json":
        click.echo(result.stdout)
    else:
        clusters = json.loads(result.stdout) if result.stdout.strip() else []
        if not clusters:
            click.echo("No clusters found.")
        else:
            click.echo("k3d Clusters:")
            for c in clusters:
                name = c.get("name", "unknown")
                servers = c.get("serversCount", 0)
                agents = c.get("agentsCount", 0)
                running = c.get("serversRunning", 0)
                click.echo(f"  {name}: {running}/{servers} servers, {agents} agents")


@cluster.command("status")
@click.option("--output", "-o", type=click.Choice(["text", "json"]), default="text",
              help="Output format")
@click.pass_context
def cluster_status(ctx, output: str):
    """Show cluster and services status."""
    cfg = ctx.obj["CONFIG"]
    work_dir = ctx.obj["WORK_DIR"]
    cluster_name = cfg["K3D_CLUSTER_NAME"]
    env = get_runtime_env(cfg)
    env["KUBECONFIG"] = str(work_dir / ".kube" / "config")

    if not check_runtime_available(cfg):
        if output == "json":
            click.echo(json.dumps({"error": "runtime_not_running"}))
        else:
            click.echo("Container runtime not running. Run: plf doctor --fix")
        sys.exit(1)

    status = {"cluster": cluster_name, "services": {}}

    # Check cluster exists
    result = subprocess.run(
        ["k3d", "cluster", "list", "-o", "json"],
        env=env, capture_output=True, text=True
    )
    clusters = json.loads(result.stdout) if result.returncode == 0 and result.stdout.strip() else []
    cluster_info = next((c for c in clusters if c.get("name") == cluster_name), None)

    if not cluster_info:
        status["state"] = "not_found"
        if output == "json":
            click.echo(json.dumps(status))
        else:
            click.echo(f"Cluster '{cluster_name}' not found.")
        sys.exit(1)

    status["state"] = "running" if cluster_info.get("serversRunning", 0) > 0 else "stopped"

    # Get pod status if cluster is running
    if status["state"] == "running":
        for ns, label in [("rustfs", "app=rustfs"), ("polaris", "app=polaris"),
                          ("polaris", "app=postgresql")]:
            result = subprocess.run(
                ["kubectl", "get", "pods", "-n", ns, "-l", label, "-o", "json"],
                env=env, capture_output=True, text=True
            )
            if result.returncode == 0:
                pods = json.loads(result.stdout).get("items", [])
                for pod in pods:
                    name = pod["metadata"]["name"]
                    phase = pod["status"].get("phase", "Unknown")
                    status["services"][name] = phase

    if output == "json":
        click.echo(json.dumps(status, indent=2))
    else:
        click.echo(f"Cluster: {cluster_name} ({status['state']})")
        if status["services"]:
            click.echo("Services:")
            for svc, state in status["services"].items():
                click.echo(f"  {svc}: {state}")
        else:
            click.echo("No services found (cluster may not be ready).")


@cli.command("teardown")
@click.option("--dry-run", "-n", is_flag=True, help="Preview without executing")
@click.option("--yes", "-y", is_flag=True, help="Skip confirmation")
@click.option("--stop-podman/--no-stop-podman", default=None,
              help="Stop Podman machine after teardown (macOS only)")
@click.pass_context
def teardown(ctx, dry_run: bool, yes: bool, stop_podman: bool | None):
    """Complete teardown - cleanup catalog, delete cluster, optionally stop Podman."""
    cfg = ctx.obj["CONFIG"]
    work_dir = ctx.obj["WORK_DIR"]
    cluster_name = cfg["K3D_CLUSTER_NAME"]
    runtime = cfg.get("PLF_CONTAINER_RUNTIME", "podman")
    machine = cfg.get("PLF_PODMAN_MACHINE", "k3d")
    is_macos = platform.system() == "Darwin"
    env = get_runtime_env(cfg)
    env["KUBECONFIG"] = str(work_dir / ".kube" / "config")

    if not yes and not dry_run:
        msg = f"Teardown will:\n  - Clean up catalog resources\n  - Delete cluster '{cluster_name}'"
        if is_macos and runtime == "podman":
            msg += f"\n  - Optionally stop Podman machine '{machine}'"
        click.echo(msg)
        if not click.confirm("Proceed?"):
            click.echo("Aborted.")
            return

    steps = [
        ("Catalog cleanup", ["uv", "run", "polaris-local-forge", "--work-dir", str(work_dir),
                            "catalog", "cleanup", "--yes"]),
        ("Delete cluster", ["k3d", "cluster", "delete", cluster_name]),
    ]

    if dry_run:
        click.echo("Would run:")
        for name, cmd in steps:
            click.echo(f"  {name}: {' '.join(cmd)}")
        if is_macos and runtime == "podman":
            click.echo(f"  Stop Podman: podman machine stop {machine} (if confirmed)")
        return

    for name, cmd in steps:
        click.echo(f"\n=== {name} ===")
        subprocess.run(cmd, env=env)

    # Handle Podman machine stop (macOS only)
    if is_macos and runtime == "podman":
        state = get_podman_machine_state(machine)
        if state == "running":
            if stop_podman is None:
                stop_podman = click.confirm(
                    f"\nStop Podman machine '{machine}' to release ports?",
                    default=True
                )
            if stop_podman:
                click.echo(f"\nStopping Podman machine '{machine}'...")
                subprocess.run(["podman", "machine", "stop", machine])
                click.echo("Podman machine stopped.")

    click.echo("\nTeardown complete.")


# =============================================================================
# Polaris Commands
# =============================================================================

@cli.group()
def polaris():
    """Polaris deployment management."""
    pass


@polaris.command("deploy")
@click.option("--dry-run", "-n", is_flag=True, help="Preview without executing")
@click.pass_context
def polaris_deploy(ctx, dry_run: bool):
    """Deploy Polaris secrets and Helm chart to the cluster."""
    work_dir = ctx.obj["WORK_DIR"]
    cfg = ctx.obj["CONFIG"]
    k8s_dir = ctx.obj["K8S_DIR"]
    env = get_runtime_env(cfg)
    env["KUBECONFIG"] = str(work_dir / ".kube" / "config")

    polaris_dir = k8s_dir / "polaris"
    polaris_chart = k8s_dir / "features" / "polaris.yaml"

    # Step 1: Apply secrets via kustomization (must be first)
    cmd_secrets = ["kubectl", "apply", "-k", str(polaris_dir)]
    if dry_run:
        click.echo(f"Would run: {' '.join(cmd_secrets)}")
    else:
        click.echo("Applying Polaris secrets...")
        result = subprocess.run(cmd_secrets, env=env)
        if result.returncode != 0:
            sys.exit(result.returncode)

    # Step 2: Apply Polaris HelmChart
    if polaris_chart.exists():
        cmd_chart = ["kubectl", "apply", "-f", str(polaris_chart)]
        if dry_run:
            click.echo(f"Would run: {' '.join(cmd_chart)}")
        else:
            click.echo("Deploying Polaris Helm chart...")
            result = subprocess.run(cmd_chart, env=env)
            sys.exit(result.returncode)
    else:
        click.echo(f"Warning: Polaris chart not found: {polaris_chart}", err=True)
        sys.exit(1)


@polaris.command("purge")
@click.option("--dry-run", "-n", is_flag=True, help="Preview without executing")
@click.pass_context
def polaris_purge(ctx, dry_run: bool):
    """Delete Polaris deployment."""
    work_dir = ctx.obj["WORK_DIR"]
    cfg = ctx.obj["CONFIG"]
    k8s_dir = ctx.obj["K8S_DIR"]
    env = get_runtime_env(cfg)
    env["KUBECONFIG"] = str(work_dir / ".kube" / "config")

    # Apply purge job
    purge_job = k8s_dir / "polaris" / "jobs" / "job-purge.yaml"
    if purge_job.exists():
        cmd = ["kubectl", "apply", "-f", str(purge_job)]
        if dry_run:
            click.echo(f"Would run: {' '.join(cmd)}")
            return
        subprocess.run(cmd, env=env)

    # Delete polaris namespace
    cmd = ["kubectl", "delete", "namespace", "polaris", "--ignore-not-found"]
    if dry_run:
        click.echo(f"Would run: {' '.join(cmd)}")
        return
    subprocess.run(cmd, env=env)


@polaris.command("bootstrap")
@click.option("--dry-run", "-n", is_flag=True, help="Preview without executing")
@click.pass_context
def polaris_bootstrap(ctx, dry_run: bool):
    """Run Polaris bootstrap job to create principal and catalog."""
    work_dir = ctx.obj["WORK_DIR"]
    cfg = ctx.obj["CONFIG"]
    k8s_dir = ctx.obj["K8S_DIR"]
    env = get_runtime_env(cfg)
    env["KUBECONFIG"] = str(work_dir / ".kube" / "config")

    bootstrap_job = k8s_dir / "polaris" / "jobs" / "job-bootstrap.yaml"
    cmd = ["kubectl", "apply", "-f", str(bootstrap_job)]

    if dry_run:
        click.echo(f"Would run: {' '.join(cmd)}")
        return

    result = subprocess.run(cmd, env=env)
    sys.exit(result.returncode)


# =============================================================================
# Catalog Commands
# =============================================================================

@cli.group()
def catalog():
    """Catalog management."""
    pass


@catalog.command("setup")
@click.option("--tags", "-t", help="Ansible tags (comma-separated)")
@click.option("--dry-run", "-n", is_flag=True, help="Preview without executing")
@click.option("--verbose", "-v", is_flag=True, help="Verbose output")
@click.pass_context
def catalog_setup(ctx, tags: str | None, dry_run: bool, verbose: bool):
    """Configure Polaris catalog via Ansible."""
    work_dir = ctx.obj["WORK_DIR"]
    exit_code = run_ansible("catalog_setup.yml", work_dir, tags=tags, dry_run=dry_run, verbose=verbose)
    sys.exit(exit_code)


@catalog.command("cleanup")
@click.option("--tags", "-t", help="Ansible tags (comma-separated)")
@click.option("--dry-run", "-n", is_flag=True, help="Preview without executing")
@click.option("--verbose", "-v", is_flag=True, help="Verbose output")
@click.option("--yes", "-y", is_flag=True, help="Skip confirmation")
@click.pass_context
def catalog_cleanup(ctx, tags: str | None, dry_run: bool, verbose: bool, yes: bool):
    """Clean up Polaris catalog via Ansible."""
    work_dir = ctx.obj["WORK_DIR"]
    if not yes and not dry_run:
        if not click.confirm("Clean up catalog?"):
            click.echo("Aborted.")
            return
    exit_code = run_ansible("catalog_cleanup.yml", work_dir, tags=tags, dry_run=dry_run, verbose=verbose)
    sys.exit(exit_code)


@catalog.command("verify-sql")
@click.pass_context
def catalog_verify_sql(ctx):
    """Run DuckDB verification using generated SQL script."""
    work_dir = ctx.obj["WORK_DIR"]
    sql_file = work_dir / "scripts" / "explore_catalog.sql"

    if not sql_file.exists():
        click.echo(f"SQL script not found: {sql_file}", err=True)
        click.echo("Run 'catalog setup' first to generate it.", err=True)
        sys.exit(1)

    if not shutil.which("duckdb"):
        click.echo("DuckDB CLI not found. Install with: brew install duckdb", err=True)
        sys.exit(1)

    click.echo(f"Running DuckDB verification from {sql_file}...")

    result = subprocess.run(
        ["duckdb", "-bail", "-init", str(sql_file), "-c", ".exit"],
        capture_output=False
    )

    if result.returncode == 0:
        click.echo("Verification completed successfully.")
    else:
        click.echo("Verification failed.", err=True)
    sys.exit(result.returncode)


@catalog.command("explore-sql")
@click.pass_context
def catalog_explore_sql(ctx):
    """Open interactive DuckDB session with catalog pre-loaded."""
    work_dir = ctx.obj["WORK_DIR"]
    sql_file = work_dir / "scripts" / "explore_catalog.sql"

    if not sql_file.exists():
        click.echo(f"SQL script not found: {sql_file}", err=True)
        click.echo("Run 'catalog setup' first to generate it.", err=True)
        sys.exit(1)

    click.echo(f"Opening interactive DuckDB with {sql_file}...")
    click.echo("Type '.exit' or Ctrl+D to quit.")

    result = subprocess.run(["duckdb", "-init", str(sql_file)])
    sys.exit(result.returncode)


# =============================================================================
# Entry Point
# =============================================================================

if __name__ == "__main__":
    cli()
