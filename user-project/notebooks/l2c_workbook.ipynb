{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2C Workbook -- Local to Cloud Migration\n",
    "\n",
    "Migrate your local **Apache Polaris** PoC to **AWS S3 + Snowflake External Iceberg Tables**.\n",
    "\n",
    "This workbook walks through the full lifecycle:\n",
    "\n",
    "1. Inspect local Polaris tables\n",
    "2. Review migration infrastructure (AWS + Snowflake)\n",
    "3. Verify synced data on S3\n",
    "4. Query from Snowflake\n",
    "5. Reset and re-demo\n",
    "6. Incremental update flow\n",
    "\n",
    "> **Prerequisites:** Run `task start WORK_DIR=<your-project>` before opening this notebook.\n",
    "> L2C sections require `./bin/plf l2c setup` to have been run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How L2C Works\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph local [Local Environment]\n",
    "        Polaris[Apache Polaris]\n",
    "        RustFS[RustFS S3]\n",
    "    end\n",
    "    subgraph cloud [Cloud]\n",
    "        S3[AWS S3 Bucket]\n",
    "        SF[Snowflake External Iceberg Table]\n",
    "    end\n",
    "    RustFS -->|\"1. sync\"| S3\n",
    "    S3 -->|\"2. register\"| SF\n",
    "    local_change[Local Data Change] -->|\"3. update --force\"| S3\n",
    "    S3 -->|\"4. refresh\"| SF\n",
    "```\n",
    "\n",
    "The `plf l2c` CLI orchestrates each step. This workbook lets you inspect\n",
    "the state at each stage and verify end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load project configuration from `.env` and L2C state from `.snow-utils/l2c-state.json`.\n",
    "Credentials are read from `work/principal.txt` (masked in output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "\n",
    "# --- .env (required) ---\n",
    "env_file = project_root / \".env\"\n",
    "if not env_file.exists():\n",
    "    print(f\"ERROR: {env_file} not found.\")\n",
    "    print(\"  Run: task prepare WORK_DIR=<your-project>\")\n",
    "    print(\"  Then: task start WORK_DIR=<your-project>\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "cfg = dotenv_values(env_file)\n",
    "has_env = True\n",
    "\n",
    "# --- work/principal.txt (needs cluster running) ---\n",
    "principal_file = project_root / \"work\" / \"principal.txt\"\n",
    "has_principal = principal_file.exists()\n",
    "realm = client_id = client_secret = None\n",
    "if has_principal:\n",
    "    lines = principal_file.read_text().strip().splitlines()\n",
    "    if len(lines) >= 3:\n",
    "        realm, client_id, client_secret = lines[0], lines[1], lines[2]\n",
    "else:\n",
    "    print(\"WARNING: work/principal.txt not found.\")\n",
    "    print(\"  Run: task start WORK_DIR=<your-project>\")\n",
    "\n",
    "# --- .snow-utils/l2c-state.json (needs plf l2c setup) ---\n",
    "state_file = project_root / \".snow-utils\" / \"l2c-state.json\"\n",
    "state = None\n",
    "has_l2c_state = state_file.exists()\n",
    "if has_l2c_state:\n",
    "    state = json.loads(state_file.read_text())\n",
    "else:\n",
    "    print(\"INFO: .snow-utils/l2c-state.json not found.\")\n",
    "    print(\"  Run: ./bin/plf l2c setup\")\n",
    "\n",
    "# --- Derived config ---\n",
    "polaris_url = cfg.get(\"POLARIS_HOST\", \"http://localhost:18181\")\n",
    "catalog_name = cfg.get(\"POLARIS_CATALOG_NAME\", \"polardb\")\n",
    "\n",
    "# --- Summary ---\n",
    "print(f\"Project root:  {project_root}\")\n",
    "print(f\"Polaris URL:   {polaris_url}\")\n",
    "print(f\"Catalog:       {catalog_name}\")\n",
    "print(f\"has_principal: {has_principal}\")\n",
    "print(f\"has_l2c_state: {has_l2c_state}\")\n",
    "if has_l2c_state and state:\n",
    "    aws = state.get(\"aws\", {})\n",
    "    sf = state.get(\"snowflake\", {})\n",
    "    print(f\"AWS bucket:    {aws.get('bucket', 'N/A')}\")\n",
    "    print(f\"AWS region:    {aws.get('region', 'N/A')}\")\n",
    "    print(f\"SF database:   {sf.get('database', 'N/A')}\")\n",
    "    print(f\"SF schema:     {sf.get('schema', 'N/A')}\")\n",
    "    print(f\"SF SA_ROLE:    {sf.get('sa_role', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Local Inventory\n",
    "\n",
    "Discover all namespaces and tables in the local Polaris catalog via the Iceberg REST API.\n",
    "These are the tables available for migration.\n",
    "\n",
    "> Skill action: `l2c-inventory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "if not has_principal:\n",
    "    print(\"\\u23ed\\ufe0f  Skipping -- Polaris credentials not available.\")\n",
    "    print(\"   Run: task start WORK_DIR=<your-project>\")\n",
    "else:\n",
    "    token_resp = requests.post(\n",
    "        f\"{polaris_url}/api/catalog/v1/oauth/tokens\",\n",
    "        data={\n",
    "            \"grant_type\": \"client_credentials\",\n",
    "            \"client_id\": client_id,\n",
    "            \"client_secret\": client_secret,\n",
    "            \"scope\": \"PRINCIPAL_ROLE:ALL\",\n",
    "        },\n",
    "        headers={\"Polaris-Realm\": realm},\n",
    "    )\n",
    "    token_resp.raise_for_status()\n",
    "    token = token_resp.json()[\"access_token\"]\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    base = f\"{polaris_url}/api/catalog\"\n",
    "\n",
    "    rows = []\n",
    "    ns_resp = requests.get(f\"{base}/v1/{catalog_name}/namespaces\", headers=headers)\n",
    "    ns_resp.raise_for_status()\n",
    "    for ns_parts in ns_resp.json().get(\"namespaces\", []):\n",
    "        ns_name = \".\".join(ns_parts)\n",
    "        tbl_resp = requests.get(f\"{base}/v1/{catalog_name}/namespaces/{ns_name}/tables\", headers=headers)\n",
    "        tbl_resp.raise_for_status()\n",
    "        for ident in tbl_resp.json().get(\"identifiers\", []):\n",
    "            table_name = ident[\"name\"]\n",
    "            meta_resp = requests.get(\n",
    "                f\"{base}/v1/{catalog_name}/namespaces/{ns_name}/tables/{table_name}\",\n",
    "                headers=headers,\n",
    "            )\n",
    "            meta_resp.raise_for_status()\n",
    "            meta = meta_resp.json()\n",
    "            location = meta.get(\"metadata\", {}).get(\"location\", \"\")\n",
    "            schemas = meta.get(\"metadata\", {}).get(\"schemas\", [])\n",
    "            col_count = len(schemas[-1].get(\"fields\", [])) if schemas else 0\n",
    "            rows.append({\n",
    "                \"namespace\": ns_name,\n",
    "                \"table\": table_name,\n",
    "                \"columns\": col_count,\n",
    "                \"location\": location,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Migration Status\n",
    "\n",
    "Current state of the L2C pipeline -- AWS infrastructure, Snowflake resources,\n",
    "and per-table sync/register status.\n",
    "\n",
    "> Requires `plf l2c setup` to have been run.  \n",
    "> Skill action: `l2c-status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not has_l2c_state:\n",
    "    print(\"\\u23ed\\ufe0f  Skipping -- L2C not configured yet.\")\n",
    "    print(\"   Run: ./bin/plf l2c setup\")\n",
    "else:\n",
    "    aws = state.get(\"aws\", {})\n",
    "    sf = state.get(\"snowflake\", {})\n",
    "\n",
    "    print(\"=== AWS ===\")\n",
    "    print(f\"  Bucket:  {aws.get('bucket', 'N/A')}\")\n",
    "    print(f\"  Region:  {aws.get('region', 'N/A')}\")\n",
    "    print(f\"  Role:    {aws.get('role_arn', 'N/A')}\")\n",
    "    print()\n",
    "    print(\"=== Snowflake ===\")\n",
    "    print(f\"  SA_ROLE:         {sf.get('sa_role', 'N/A')}\")\n",
    "    print(f\"  Ext Volume:      {sf.get('external_volume', 'N/A')}\")\n",
    "    print(f\"  Catalog Int:     {sf.get('catalog_integration', 'N/A')}\")\n",
    "    print(f\"  Database.Schema: {sf.get('database', 'N/A')}.{sf.get('schema', 'N/A')}\")\n",
    "    print()\n",
    "\n",
    "    tables = state.get(\"tables\", {})\n",
    "    if tables:\n",
    "        rows = []\n",
    "        for key, info in tables.items():\n",
    "            rows.append({\n",
    "                \"table\": key,\n",
    "                \"namespace\": info.get(\"namespace\", \"\"),\n",
    "                \"sync_status\": info.get(\"sync\", {}).get(\"status\", \"pending\"),\n",
    "                \"last_sync\": info.get(\"sync\", {}).get(\"last_sync\", \"\"),\n",
    "                \"register_status\": info.get(\"register\", {}).get(\"status\", \"pending\"),\n",
    "            })\n",
    "        df = pd.DataFrame(rows)\n",
    "        df\n",
    "    else:\n",
    "        print(\"No tables in state yet. Run: ./bin/plf l2c migrate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sync Verification\n",
    "\n",
    "Compare object counts between local RustFS and the AWS S3 migration bucket.\n",
    "A matching count confirms all Iceberg data files and metadata were synced.\n",
    "\n",
    "> Requires `plf l2c migrate` to have been run.  \n",
    "> Skill action: `l2c-verify`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from botocore.config import Config as BotoConfig\n",
    "\n",
    "if not has_l2c_state:\n",
    "    print(\"\\u23ed\\ufe0f  Skipping -- L2C not configured yet.\")\n",
    "    print(\"   Run: ./bin/plf l2c setup\")\n",
    "else:\n",
    "    aws = state.get(\"aws\", {})\n",
    "    bucket = aws.get(\"bucket\", \"\")\n",
    "    region = aws.get(\"region\", \"us-east-1\")\n",
    "    profile = aws.get(\"profile\", \"default\")\n",
    "\n",
    "    # Local RustFS S3\n",
    "    rustfs_s3 = boto3.Session(\n",
    "        aws_access_key_id=cfg.get(\"AWS_ACCESS_KEY_ID\", \"admin\"),\n",
    "        aws_secret_access_key=cfg.get(\"AWS_SECRET_ACCESS_KEY\", \"password\"),\n",
    "        region_name=\"us-east-1\",\n",
    "    ).client(\n",
    "        \"s3\",\n",
    "        endpoint_url=cfg.get(\"AWS_ENDPOINT_URL\", \"http://localhost:19000\"),\n",
    "        region_name=\"us-east-1\",\n",
    "        config=BotoConfig(s3={\"addressing_style\": \"path\"}),\n",
    "    )\n",
    "\n",
    "    # Real AWS S3\n",
    "    cloud_s3 = boto3.Session(profile_name=profile, region_name=region).client(\"s3\", region_name=region)\n",
    "\n",
    "    def count_objects(s3_client, bucket_name, prefix=\"\"):\n",
    "        paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "        count = 0\n",
    "        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "            count += page.get(\"KeyCount\", 0)\n",
    "        return count\n",
    "\n",
    "    local_bucket = cfg.get(\"POLARIS_CATALOG_NAME\", \"polardb\")\n",
    "    rows = []\n",
    "    for key, info in state.get(\"tables\", {}).items():\n",
    "        ns = info.get(\"namespace\", \"\")\n",
    "        tbl = info.get(\"table\", \"\")\n",
    "        prefix = f\"{ns}/{tbl}/\"\n",
    "        local_count = count_objects(rustfs_s3, local_bucket, prefix)\n",
    "        s3_count = count_objects(cloud_s3, bucket, prefix)\n",
    "        rows.append({\n",
    "            \"table\": key,\n",
    "            \"local_objects\": local_count,\n",
    "            \"s3_objects\": s3_count,\n",
    "            \"match\": \"Yes\" if local_count == s3_count else \"NO\",\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query from Snowflake\n",
    "\n",
    "Run the same count query both **locally** (PyIceberg) and in **Snowflake** to\n",
    "confirm the data matches end-to-end.\n",
    "\n",
    "Uses `snowflake-connector-python` with a named connection from\n",
    "`~/.snowflake/connections.toml` -- no hardcoded credentials needed.\n",
    "\n",
    "> Requires `plf l2c migrate` to have been run.  \n",
    "> Skill action: `l2c-query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not has_l2c_state:\n",
    "    print(\"\\u23ed\\ufe0f  Skipping -- L2C not configured yet.\")\n",
    "    print(\"   Run: ./bin/plf l2c setup\")\n",
    "else:\n",
    "    import snowflake.connector\n",
    "    from pyiceberg.catalog import load_catalog\n",
    "\n",
    "    sf = state[\"snowflake\"]\n",
    "    database = sf[\"database\"]\n",
    "    schema = sf[\"schema\"]\n",
    "\n",
    "    # Snowflake query\n",
    "    conn = snowflake.connector.connect(\n",
    "        connection_name=cfg.get(\"SNOWFLAKE_DEFAULT_CONNECTION_NAME\", \"default\"),\n",
    "        role=sf[\"sa_role\"],\n",
    "        database=database,\n",
    "        schema=schema,\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for key, info in state.get(\"tables\", {}).items():\n",
    "        if info.get(\"register\", {}).get(\"status\") != \"done\":\n",
    "            continue\n",
    "        sf_table = key.upper()\n",
    "\n",
    "        # Snowflake count\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {database}.{schema}.{sf_table}\")\n",
    "        sf_count = cur.fetchone()[0]\n",
    "\n",
    "        # Local count via PyIceberg\n",
    "        ns = info[\"namespace\"]\n",
    "        tbl = info[\"table\"]\n",
    "        local_catalog = load_catalog(\n",
    "            \"polaris\",\n",
    "            **{\n",
    "                \"type\": \"rest\",\n",
    "                \"uri\": polaris_url,\n",
    "                \"credential\": f\"{client_id}:{client_secret}\",\n",
    "                \"warehouse\": catalog_name,\n",
    "                \"scope\": \"PRINCIPAL_ROLE:ALL\",\n",
    "                \"header.X-Iceberg-Access-Delegation\": \"vended-credentials\",\n",
    "                \"header.Polaris-Realm\": realm,\n",
    "            },\n",
    "        )\n",
    "        iceberg_tbl = local_catalog.load_table(f\"{ns}.{tbl}\")\n",
    "        local_count = len(iceberg_tbl.scan().to_arrow())\n",
    "\n",
    "        rows.append({\n",
    "            \"table\": sf_table,\n",
    "            \"local_count\": local_count,\n",
    "            \"snowflake_count\": sf_count,\n",
    "            \"match\": \"Yes\" if local_count == sf_count else \"NO\",\n",
    "        })\n",
    "\n",
    "    conn.close()\n",
    "    df = pd.DataFrame(rows)\n",
    "    df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reset and Reload (Demo Reset)\n",
    "\n",
    "Drop and recreate the local catalog to regenerate sample data, then clear\n",
    "the S3 bucket and Snowflake tables. This lets you re-run the full L2C\n",
    "migration demo **without** tearing down the k3d cluster.\n",
    "\n",
    "> Skill action: `l2c-reset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "if not has_principal:\n",
    "    print(\"\\u23ed\\ufe0f  Skipping -- Polaris credentials not available.\")\n",
    "    print(\"   Run: task start WORK_DIR=<your-project>\")\n",
    "else:\n",
    "    plf = str(project_root / \"bin\" / \"plf\")\n",
    "\n",
    "    print(\"=== Step 1: Drop and recreate catalog ===\")\n",
    "    subprocess.run([plf, \"catalog\", \"cleanup\", \"--yes\"], cwd=project_root, check=True)\n",
    "    subprocess.run([plf, \"catalog\", \"setup\"], cwd=project_root, check=True)\n",
    "    print(\"Catalog recreated with sample data.\")\n",
    "    print()\n",
    "\n",
    "    if has_l2c_state:\n",
    "        print(\"=== Step 2: Clear S3 + Snowflake tables ===\")\n",
    "        subprocess.run([plf, \"l2c\", \"clear\", \"--yes\"], cwd=project_root, check=True)\n",
    "        print(\"S3 objects and Snowflake tables cleared.\")\n",
    "    else:\n",
    "        print(\"L2C not configured -- skipping S3/Snowflake clear.\")\n",
    "\n",
    "    print()\n",
    "    print(\"Ready to re-run L2C migration demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Incremental Update\n",
    "\n",
    "Demonstrate the day-2 workflow:\n",
    "\n",
    "1. Insert new rows into the **local** Polaris table\n",
    "2. Run `plf l2c update --force` to sync changes to S3 and refresh Snowflake\n",
    "3. Query Snowflake to confirm the new rows appear\n",
    "\n",
    "This proves that the L2C bridge works for ongoing development, not just\n",
    "one-shot migrations.\n",
    "\n",
    "> Requires both local cluster and `plf l2c setup` to have been run.  \n",
    "> Skill action: `l2c-update`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "if not has_l2c_state or not has_principal:\n",
    "    missing = []\n",
    "    if not has_principal:\n",
    "        missing.append(\"Polaris credentials (run: task start)\")\n",
    "    if not has_l2c_state:\n",
    "        missing.append(\"L2C state (run: ./bin/plf l2c setup)\")\n",
    "    print(f\"\\u23ed\\ufe0f  Skipping -- missing: {', '.join(missing)}\")\n",
    "else:\n",
    "    from pyiceberg.catalog import load_catalog\n",
    "    import snowflake.connector\n",
    "\n",
    "    sf = state[\"snowflake\"]\n",
    "    database = sf[\"database\"]\n",
    "    schema = sf[\"schema\"]\n",
    "\n",
    "    # Pick the first registered table for the demo\n",
    "    demo_key = None\n",
    "    demo_info = None\n",
    "    for key, info in state.get(\"tables\", {}).items():\n",
    "        if info.get(\"register\", {}).get(\"status\") == \"done\":\n",
    "            demo_key = key\n",
    "            demo_info = info\n",
    "            break\n",
    "\n",
    "    if not demo_key:\n",
    "        print(\"No registered tables found. Run: ./bin/plf l2c migrate\")\n",
    "    else:\n",
    "        ns = demo_info[\"namespace\"]\n",
    "        tbl = demo_info[\"table\"]\n",
    "        sf_table = demo_key.upper()\n",
    "\n",
    "        # Connect to local catalog\n",
    "        local_catalog = load_catalog(\n",
    "            \"polaris\",\n",
    "            **{\n",
    "                \"type\": \"rest\",\n",
    "                \"uri\": polaris_url,\n",
    "                \"credential\": f\"{client_id}:{client_secret}\",\n",
    "                \"warehouse\": catalog_name,\n",
    "                \"scope\": \"PRINCIPAL_ROLE:ALL\",\n",
    "                \"header.X-Iceberg-Access-Delegation\": \"vended-credentials\",\n",
    "                \"header.Polaris-Realm\": realm,\n",
    "            },\n",
    "        )\n",
    "        iceberg_tbl = local_catalog.load_table(f\"{ns}.{tbl}\")\n",
    "        before_count = len(iceberg_tbl.scan().to_arrow())\n",
    "\n",
    "        # Insert a few demo rows by appending a PyArrow table\n",
    "        existing = iceberg_tbl.scan().to_arrow()\n",
    "        sample = existing.slice(0, min(3, len(existing)))\n",
    "        iceberg_tbl.append(sample)\n",
    "        after_local = len(iceberg_tbl.scan().to_arrow())\n",
    "        print(f\"Local {ns}.{tbl}: {before_count} -> {after_local} rows\")\n",
    "\n",
    "        # Sync to cloud\n",
    "        plf = str(project_root / \"bin\" / \"plf\")\n",
    "        print(\"\\nRunning: plf l2c update --force --yes\")\n",
    "        subprocess.run([plf, \"l2c\", \"update\", \"--force\", \"--yes\"], cwd=project_root, check=True)\n",
    "\n",
    "        # Query Snowflake\n",
    "        conn = snowflake.connector.connect(\n",
    "            connection_name=cfg.get(\"SNOWFLAKE_DEFAULT_CONNECTION_NAME\", \"default\"),\n",
    "            role=sf[\"sa_role\"],\n",
    "            database=database,\n",
    "            schema=schema,\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(f\"SELECT COUNT(*) FROM {database}.{schema}.{sf_table}\")\n",
    "        sf_count = cur.fetchone()[0]\n",
    "        conn.close()\n",
    "\n",
    "        df = pd.DataFrame([{\n",
    "            \"table\": sf_table,\n",
    "            \"before\": before_count,\n",
    "            \"after_local\": after_local,\n",
    "            \"after_snowflake\": sf_count,\n",
    "            \"match\": \"Yes\" if after_local == sf_count else \"NO\",\n",
    "        }])\n",
    "        df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
