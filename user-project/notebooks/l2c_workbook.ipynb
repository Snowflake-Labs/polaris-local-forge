{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# L2C Workbook -- Local to Cloud Migration\n",
        "\n",
        "Migrate your local **Apache Polaris** PoC to **AWS S3 + Snowflake External Iceberg Tables**.\n",
        "\n",
        "This workbook walks through the full lifecycle:\n",
        "\n",
        "1. Inspect local Polaris tables\n",
        "2. Review migration infrastructure (AWS + Snowflake)\n",
        "3. Verify synced data on S3\n",
        "4. Query from Snowflake\n",
        "5. Incremental update flow\n",
        "6. Reset and re-demo\n",
        "\n",
        "> **Prerequisites:**\n",
        "> * Run `task setup:all WORK_DIR=<your-project>` before opening this notebook.\n",
        "> * L2C sections require `$PROJECT_HOME/bin/plf l2c setup` to have been run.\n",
        "> * **AWS Credentials:** Ensure AWS credentials are properly configured for S3 access:\n",
        ">   - **AWS SSO:** Run `aws sso login --profile <your-profile>` if using SSO\n",
        ">   - **AWS Profile:** Set `AWS_PROFILE=<your-profile>` or configure default credentials\n",
        ">   - **Verify Access:** Test with `aws sts get-caller-identity` to confirm authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How L2C Works\n",
        "\n",
        "**L2C Migration Flow:**\n",
        "\n",
        "**INITIAL SETUP:**\n",
        "1. **Local Apache Polaris** (Iceberg REST) stores tables via RustFS (S3-compatible)\n",
        "2. **AWS S3 bucket** configured for Iceberg storage  \n",
        "3. **Snowflake External Volume** points to S3 bucket\n",
        "\n",
        "**MIGRATION STEPS:**\n",
        "1. **SYNC:** Copy Iceberg data/metadata from local RustFS to AWS S3\n",
        "2. **REGISTER:** Create Snowflake External Iceberg Tables pointing to S3 paths\n",
        "3. **QUERY:** Access data through Snowflake while maintaining Iceberg format\n",
        "\n",
        "**DAY-2 OPERATIONS:**\n",
        "1. **UPDATE:** Local table changes ‚Üí sync to S3 ‚Üí refresh Snowflake metadata\n",
        "2. **VERIFY:** Query both local and Snowflake to confirm consistency\n",
        "\n",
        "The `plf l2c` CLI orchestrates each step. This workbook lets you inspect\n",
        "the state at each stage and verify end-to-end.\n",
        "\n",
        "> **‚ö†Ô∏è AWS Credentials Required:** L2C migration requires active AWS credentials with S3 access.\n",
        "> The notebook will fail at sync/verification steps if AWS authentication is not properly configured.\n",
        "> Common issues:\n",
        "> - **SSO Expired:** Re-run `aws sso login --profile <your-profile>`\n",
        "> - **Profile Missing:** Check `~/.aws/config` and set `AWS_PROFILE` if needed\n",
        "> - **Permissions:** Ensure your AWS role has S3 read/write access to the configured bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Load project configuration from `.env` and L2C state from `.snow-utils/l2c-state.json`.\n",
        "Credentials are read from `work/principal.txt` (masked in output)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from dotenv import dotenv_values\n",
        "\n",
        "project_root = Path(\"..\").resolve()\n",
        "\n",
        "# --- .env (required) ---\n",
        "env_file = project_root / \".env\"\n",
        "if not env_file.exists():\n",
        "    print(f\"ERROR: {env_file} not found.\")\n",
        "    print(\"  Run: task setup:all WORK_DIR=<your-project>\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "cfg = dotenv_values(env_file)\n",
        "has_env = True\n",
        "\n",
        "# --- work/principal.txt (needs cluster running) ---\n",
        "principal_file = project_root / \"work\" / \"principal.txt\"\n",
        "has_principal = principal_file.exists()\n",
        "realm = client_id = client_secret = None\n",
        "if has_principal:\n",
        "    content = principal_file.read_text().strip()\n",
        "    if \",\" in content:\n",
        "        # Comma-separated format: REALM,CLIENT_ID,CLIENT_SECRET\n",
        "        realm, client_id, client_secret = content.split(\",\")\n",
        "    else:\n",
        "        # Line-separated format (fallback)\n",
        "        lines = content.splitlines()\n",
        "        if len(lines) >= 3:\n",
        "            realm, client_id, client_secret = lines[0], lines[1], lines[2]\n",
        "else:\n",
        "    print(\"WARNING: work/principal.txt not found.\")\n",
        "    print(\"  Run: task setup:all WORK_DIR=<your-project>\")\n",
        "\n",
        "# --- .snow-utils/l2c-state.json (needs plf l2c setup) ---\n",
        "state_file = project_root / \".snow-utils\" / \"l2c-state.json\"\n",
        "state = None\n",
        "has_l2c_state = state_file.exists()\n",
        "if has_l2c_state:\n",
        "    state = json.loads(state_file.read_text())\n",
        "else:\n",
        "    print(\"INFO: .snow-utils/l2c-state.json not found.\")\n",
        "    print(f\"  Run: {project_root}/bin/plf l2c setup\")\n",
        "\n",
        "# --- Derived config ---\n",
        "polaris_url = cfg.get(\"POLARIS_HOST\", \"http://localhost:18181\")\n",
        "catalog_name = cfg.get(\"PLF_POLARIS_CATALOG_NAME\", \"polardb\")\n",
        "\n",
        "# --- Summary ---\n",
        "print(f\"Project root:  {project_root}\")\n",
        "print(f\"Polaris URL:   {polaris_url}\")\n",
        "print(f\"Catalog:       {catalog_name}\")\n",
        "print(f\"has_principal: {has_principal}\")\n",
        "print(f\"has_l2c_state: {has_l2c_state}\")\n",
        "if has_l2c_state and state:\n",
        "    aws = state.get(\"aws\", {})\n",
        "    sf = state.get(\"snowflake\", {})\n",
        "    print(f\"AWS bucket:    {aws.get('bucket', 'N/A')}\")\n",
        "    print(f\"AWS region:    {aws.get('region', 'N/A')}\")\n",
        "    print(f\"SF database:   {sf.get('database', 'N/A')}\")\n",
        "    print(f\"SF schema:     {sf.get('schema', 'N/A')}\")\n",
        "    print(f\"SF SA_ROLE:    {sf.get('sa_role', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions\n",
        "\n",
        "Reusable functions to eliminate code duplication across sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import duckdb\n",
        "import snowflake.connector\n",
        "\n",
        "def get_polaris_token(polaris_url, client_id, client_secret, realm):\n",
        "    \"\"\"Get OAuth token from Polaris REST API.\"\"\"\n",
        "    token_resp = requests.post(\n",
        "        f\"{polaris_url}/api/catalog/v1/oauth/tokens\",\n",
        "        data={\n",
        "            \"grant_type\": \"client_credentials\",\n",
        "            \"client_id\": client_id,\n",
        "            \"client_secret\": client_secret,\n",
        "            \"scope\": \"PRINCIPAL_ROLE:ALL\",\n",
        "        },\n",
        "        headers={\"Polaris-Realm\": realm},\n",
        "    )\n",
        "    token_resp.raise_for_status()\n",
        "    return token_resp.json()[\"access_token\"]\n",
        "\n",
        "def create_polaris_headers(polaris_url, client_id, client_secret, realm):\n",
        "    \"\"\"Create headers for Polaris REST API calls.\"\"\"\n",
        "    token = get_polaris_token(polaris_url, client_id, client_secret, realm)\n",
        "    return {\n",
        "        \"Authorization\": f\"Bearer {token}\", \n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "def setup_duckdb_polaris_connection(polaris_url, catalog_name, client_id, client_secret, realm):\n",
        "    \"\"\"Setup DuckDB with Polaris Iceberg catalog connection.\"\"\"\n",
        "    conn_ddb = duckdb.connect(\":memory:\")\n",
        "    \n",
        "    # Install and load iceberg extension\n",
        "    conn_ddb.execute(\"INSTALL iceberg\")\n",
        "    conn_ddb.execute(\"LOAD iceberg\")\n",
        "    \n",
        "    # Create secret for OAuth2 authentication\n",
        "    oauth_server = f\"{polaris_url}/api/catalog/v1/oauth/tokens\"\n",
        "    conn_ddb.execute(f\"\"\"\n",
        "        CREATE OR REPLACE SECRET polaris_secret (\n",
        "            TYPE iceberg,\n",
        "            CLIENT_ID '{client_id}',\n",
        "            CLIENT_SECRET '{client_secret}',\n",
        "            OAUTH2_SERVER_URI '{oauth_server}'\n",
        "        )\n",
        "    \"\"\")\n",
        "    \n",
        "    # Attach catalog\n",
        "    catalog_endpoint = f\"{polaris_url}/api/catalog\"\n",
        "    conn_ddb.execute(f\"\"\"\n",
        "        ATTACH '{catalog_name}' AS polaris_catalog (\n",
        "            TYPE iceberg,\n",
        "            SECRET polaris_secret,\n",
        "            ENDPOINT '{catalog_endpoint}'\n",
        "        )\n",
        "    \"\"\")\n",
        "    \n",
        "    return conn_ddb\n",
        "\n",
        "def get_table_count_via_duckdb(polaris_url, catalog_name, client_id, client_secret, realm, namespace, table):\n",
        "    \"\"\"Get row count for a table via DuckDB.\"\"\"\n",
        "    try:\n",
        "        conn_ddb = setup_duckdb_polaris_connection(polaris_url, catalog_name, client_id, client_secret, realm)\n",
        "        result = conn_ddb.execute(f\"SELECT COUNT(*) FROM polaris_catalog.{namespace}.{table}\").fetchone()\n",
        "        count = result[0] if result else 0\n",
        "        conn_ddb.close()\n",
        "        return count\n",
        "    except Exception as e:\n",
        "        print(f\"DuckDB query failed for {namespace}.{table}: {e}\")\n",
        "        return \"Error\"\n",
        "\n",
        "def create_snowflake_connection(cfg, sf_config):\n",
        "    \"\"\"Create Snowflake connection with error handling.\"\"\"\n",
        "    try:\n",
        "        connection_name = cfg.get(\"SNOWFLAKE_DEFAULT_CONNECTION_NAME\", \"default\")\n",
        "        print(f\"Attempting Snowflake connection using: {connection_name}\")\n",
        "        \n",
        "        conn = snowflake.connector.connect(\n",
        "            connection_name=connection_name,\n",
        "            role=sf_config[\"sa_role\"],\n",
        "            database=sf_config[\"database\"],\n",
        "            schema=sf_config[\"schema\"],\n",
        "            login_timeout=10,  # 10 second timeout\n",
        "        )\n",
        "        print(f\"‚úÖ Snowflake connection successful\")\n",
        "        return conn, True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Snowflake connection failed: {e}\")\n",
        "        if \"browser\" in str(e).lower() or \"oauth\" in str(e).lower():\n",
        "            print(f\"   Connection '{connection_name}' requires browser authentication\")\n",
        "            print(f\"   For notebook use, consider configuring a key-pair connection\")\n",
        "        print(f\"   Check ~/.snowflake/connections.toml configuration\")\n",
        "        return None, False\n",
        "\n",
        "print(\"‚úÖ Utility functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Local Inventory\n",
        "\n",
        "Discover all namespaces and tables in the local Polaris catalog via the Iceberg REST API.\n",
        "These are the tables available for migration.\n",
        "\n",
        "<!-- Skill action: l2c-inventory -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "if not has_principal:\n",
        "    print(\"\\u23ed\\ufe0f  Skipping -- Polaris credentials not available.\")\n",
        "    print(\"   Run: task setup:all WORK_DIR=<your-project>\")\n",
        "else:\n",
        "    import requests\n",
        "    headers = create_polaris_headers(polaris_url, client_id, client_secret, realm)\n",
        "    base = f\"{polaris_url}/api/catalog\"\n",
        "\n",
        "    rows = []\n",
        "    ns_resp = requests.get(f\"{base}/v1/{catalog_name}/namespaces\", headers=headers)\n",
        "    ns_resp.raise_for_status()\n",
        "    for ns_parts in ns_resp.json().get(\"namespaces\", []):\n",
        "        ns_name = \".\".join(ns_parts)\n",
        "        tbl_resp = requests.get(f\"{base}/v1/{catalog_name}/namespaces/{ns_name}/tables\", headers=headers)\n",
        "        tbl_resp.raise_for_status()\n",
        "        for ident in tbl_resp.json().get(\"identifiers\", []):\n",
        "            table_name = ident[\"name\"]\n",
        "            meta_resp = requests.get(\n",
        "                f\"{base}/v1/{catalog_name}/namespaces/{ns_name}/tables/{table_name}\",\n",
        "                headers=headers,\n",
        "            )\n",
        "            meta_resp.raise_for_status()\n",
        "            meta = meta_resp.json()\n",
        "            location = meta.get(\"metadata\", {}).get(\"location\", \"\")\n",
        "            schemas = meta.get(\"metadata\", {}).get(\"schemas\", [])\n",
        "            col_count = len(schemas[-1].get(\"fields\", [])) if schemas else 0\n",
        "            rows.append({\n",
        "                \"namespace\": ns_name,\n",
        "                \"table\": table_name,\n",
        "                \"columns\": col_count,\n",
        "                \"location\": location,\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Migration Status\n",
        "\n",
        "Current state of the L2C pipeline -- AWS infrastructure, Snowflake resources,\n",
        "and per-table sync/register status.\n",
        "\n",
        "> Requires `$PROJECT_HOME/bin/plf l2c setup` to have been run.  \n",
        "<!-- Skill action: l2c-status -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "if not has_l2c_state:\n",
        "    print(\"\\u23ed\\ufe0f  Skipping -- L2C not configured yet.\")\n",
        "    print(f\"   Run: {project_root}/bin/plf l2c setup\")\n",
        "else:\n",
        "    aws = state.get(\"aws\", {})\n",
        "    sf = state.get(\"snowflake\", {})\n",
        "\n",
        "    print(\"=== AWS ===\")\n",
        "    print(f\"  Bucket:  {aws.get('bucket', 'N/A')}\")\n",
        "    print(f\"  Region:  {aws.get('region', 'N/A')}\")\n",
        "    print(f\"  Role:    {aws.get('role_arn', 'N/A')}\")\n",
        "    print()\n",
        "    print(\"=== Snowflake ===\")\n",
        "    print(f\"  SA_ROLE:         {sf.get('sa_role', 'N/A')}\")\n",
        "    print(f\"  Ext Volume:      {sf.get('external_volume', 'N/A')}\")\n",
        "    print(f\"  Catalog Int:     {sf.get('catalog_integration', 'N/A')}\")\n",
        "    print(f\"  Database.Schema: {sf.get('database', 'N/A')}.{sf.get('schema', 'N/A')}\")\n",
        "    print()\n",
        "\n",
        "    tables = state.get(\"tables\", {})\n",
        "    if tables:\n",
        "        rows = []\n",
        "        for key, info in tables.items():\n",
        "            rows.append({\n",
        "                \"table\": key,\n",
        "                \"namespace\": info.get(\"namespace\", \"\"),\n",
        "                \"sync_status\": info.get(\"sync\", {}).get(\"status\", \"pending\"),\n",
        "                \"last_sync\": info.get(\"sync\", {}).get(\"last_sync\", \"\"),\n",
        "                \"register_status\": info.get(\"register\", {}).get(\"status\", \"pending\"),\n",
        "            })\n",
        "        df = pd.DataFrame(rows)\n",
        "        display(df)\n",
        "    else:\n",
        "        print(f\"No tables in state yet. Run: {project_root}/bin/plf l2c migrate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sync Verification\n",
        "\n",
        "Compare object counts between local RustFS and the AWS S3 migration bucket.\n",
        "A matching count confirms all Iceberg data files and metadata were synced.\n",
        "\n",
        "> Requires `$PROJECT_HOME/bin/plf l2c migrate` to have been run.  \n",
        "<!-- Skill action: l2c-verify -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "import os\n",
        "from contextlib import contextmanager\n",
        "from botocore.config import Config as BotoConfig\n",
        "\n",
        "if not has_l2c_state:\n",
        "    print(\"\\u23ed\\ufe0f  Skipping -- L2C not configured yet.\")\n",
        "    print(f\"   Run: {project_root}/bin/plf l2c setup\")\n",
        "else:\n",
        "    # Duplicate L2C session functions (same as CLI implementation)\n",
        "    _AWS_ENV_VARS = [\n",
        "        \"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\", \"AWS_SESSION_TOKEN\",\n",
        "        \"AWS_ENDPOINT_URL\", \"AWS_DEFAULT_REGION\", \"AWS_REGION\",\n",
        "        \"AWS_PROFILE\", \"AWS_DEFAULT_PROFILE\",\n",
        "        \"AWS_CONFIG_FILE\", \"AWS_SHARED_CREDENTIALS_FILE\",\n",
        "    ]\n",
        "\n",
        "    @contextmanager\n",
        "    def scrubbed_aws_env():\n",
        "        \"\"\"Context manager that strips RustFS AWS_* vars for the entire block.\"\"\"\n",
        "        saved = {k: os.environ.pop(k) for k in _AWS_ENV_VARS if k in os.environ}\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            os.environ.update(saved)\n",
        "\n",
        "    def create_rustfs_session(cfg: dict):\n",
        "        \"\"\"Create an isolated boto3 S3 client for local RustFS.\"\"\"\n",
        "        session = boto3.Session(\n",
        "            aws_access_key_id=cfg.get(\"AWS_ACCESS_KEY_ID\", \"admin\"),\n",
        "            aws_secret_access_key=cfg.get(\"AWS_SECRET_ACCESS_KEY\", \"password\"),\n",
        "            region_name=\"us-east-1\",\n",
        "        )\n",
        "        endpoint = cfg.get(\"AWS_ENDPOINT_URL\", \"http://localhost:19000\")\n",
        "        return session.client(\n",
        "            \"s3\",\n",
        "            endpoint_url=endpoint,\n",
        "            region_name=\"us-east-1\",\n",
        "            config=BotoConfig(s3={\"addressing_style\": \"path\"}),\n",
        "        )\n",
        "\n",
        "    def _resolve_profile(aws_profile: str | None = None) -> str:\n",
        "        \"\"\"Resolve AWS profile: CLI flag > env var > default.\"\"\"\n",
        "        return aws_profile or os.environ.get(\"L2C_AWS_PROFILE\") or \"default\"\n",
        "\n",
        "    def create_cloud_session(aws_profile: str | None = None, region: str = \"us-east-1\"):\n",
        "        \"\"\"Create an isolated boto3 session for real AWS.\"\"\"\n",
        "        if aws_profile is None:\n",
        "            # No profile specified - use default credential chain (env vars, SSO, etc.)\n",
        "            session = boto3.Session(region_name=region)\n",
        "            profile_name = \"default-chain\"\n",
        "        else:\n",
        "            # Specific profile requested\n",
        "            profile = _resolve_profile(aws_profile)\n",
        "            session = boto3.Session(profile_name=profile, region_name=region)\n",
        "            profile_name = profile\n",
        "            \n",
        "        cloud_s3 = session.client(\"s3\", region_name=region)\n",
        "        \n",
        "        # Get account info for verification (like CLI does)\n",
        "        cloud_sts = session.client(\"sts\")\n",
        "        account_id = cloud_sts.get_caller_identity()[\"Account\"]\n",
        "        print(f\"Cloud session: AWS account {account_id}, profile '{profile_name}', region '{region}'\")\n",
        "        \n",
        "        return cloud_s3, None, cloud_sts\n",
        "\n",
        "    # Use L2C state for configuration\n",
        "    aws = state.get(\"aws\", {})\n",
        "    bucket = aws.get(\"bucket\", \"\")\n",
        "    region = aws.get(\"region\", \"us-west-2\")\n",
        "    profile = aws.get(\"profile\")  # Can be None\n",
        "\n",
        "    # Use exact same session creation as L2C CLI - scrub environment for both\n",
        "    with scrubbed_aws_env():\n",
        "        rustfs_s3 = create_rustfs_session(cfg)\n",
        "\n",
        "    # Real AWS S3 - use exact same pattern as L2C CLI\n",
        "    try:\n",
        "        with scrubbed_aws_env():\n",
        "            cloud_s3, _, _ = create_cloud_session(profile, region)\n",
        "        aws_available = True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  AWS session creation failed: {e}\")\n",
        "        print(f\"   Run: {project_root}/bin/plf l2c setup\")\n",
        "        print(f\"   Or check AWS credentials: aws sts get-caller-identity\")\n",
        "        aws_available = False\n",
        "\n",
        "    def count_objects(s3_client, bucket_name, prefix=\"\"):\n",
        "        try:\n",
        "            paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
        "            count = 0\n",
        "            for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
        "                count += page.get(\"KeyCount\", 0)\n",
        "            return count\n",
        "        except Exception:\n",
        "            return -1  # Error indicator\n",
        "\n",
        "    local_bucket = cfg.get(\"POLARIS_CATALOG_NAME\", \"polardb\")\n",
        "    rows = []\n",
        "    for key, info in state.get(\"tables\", {}).items():\n",
        "        ns = info.get(\"namespace\", \"\")\n",
        "        tbl = info.get(\"table\", \"\")\n",
        "        prefix = f\"{ns}/{tbl}/\"\n",
        "        local_count = count_objects(rustfs_s3, local_bucket, prefix)\n",
        "        \n",
        "        if aws_available:\n",
        "            s3_count = count_objects(cloud_s3, bucket, prefix)\n",
        "            match_status = \"Yes\" if local_count == s3_count else \"NO\"\n",
        "        else:\n",
        "            s3_count = \"N/A\"\n",
        "            match_status = \"AWS not configured\"\n",
        "            \n",
        "        rows.append({\n",
        "            \"table\": key,\n",
        "            \"local_objects\": local_count,\n",
        "            \"s3_objects\": s3_count,\n",
        "            \"match\": match_status,\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Query from Snowflake\n",
        "\n",
        "Run the same count query both **locally** (PyIceberg) and in **Snowflake** to\n",
        "confirm the data matches end-to-end.\n",
        "\n",
        "Uses `snowflake-connector-python` with a named connection from\n",
        "`~/.snowflake/connections.toml` -- no hardcoded credentials needed.\n",
        "\n",
        "> Requires `$PROJECT_HOME/bin/plf l2c migrate` to have been run.  \n",
        "<!-- Skill action: l2c-query -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "if not has_l2c_state:\n",
        "    print(\"\\u23ed\\ufe0f  Skipping -- L2C not configured yet.\")\n",
        "    print(f\"   Run: {project_root}/bin/plf l2c setup\")\n",
        "else:\n",
        "    import snowflake.connector\n",
        "\n",
        "    sf = state[\"snowflake\"]\n",
        "    database = sf[\"database\"]\n",
        "    schema = sf[\"schema\"]\n",
        "\n",
        "    # Snowflake query - handle OAuth connections gracefully\n",
        "    # Use utility function for Snowflake connection\n",
        "    conn, snowflake_available = create_snowflake_connection(cfg, sf)\n",
        "\n",
        "    rows = []\n",
        "    if snowflake_available:\n",
        "        for key, info in state.get(\"tables\", {}).items():\n",
        "            if info.get(\"register\", {}).get(\"status\") != \"done\":\n",
        "                continue\n",
        "            # Use the fully qualified Snowflake table name from register section\n",
        "            sf_table = info.get(\"register\", {}).get(\n",
        "                \"sf_table\", f\"{database}.{schema}.{key.upper()}\"\n",
        "            )\n",
        "\n",
        "            # Snowflake count\n",
        "            cur = conn.cursor()\n",
        "            cur.execute(f\"SELECT COUNT(*) FROM {sf_table}\")\n",
        "            sf_count = cur.fetchone()[0]\n",
        "\n",
        "            # Local count using DuckDB (much simpler and more reliable!)\n",
        "            ns = info[\"namespace\"]\n",
        "            tbl = info[\"table\"]\n",
        "\n",
        "            local_count = get_table_count_via_duckdb(\n",
        "                polaris_url,\n",
        "                catalog_name,\n",
        "                client_id,\n",
        "                client_secret,\n",
        "                realm,\n",
        "                ns,\n",
        "                tbl,\n",
        "            )\n",
        "\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"table\": f\"{ns}.{tbl}\",\n",
        "                    \"local_count\": local_count,\n",
        "                    \"snowflake_count\": sf_count,\n",
        "                    \"match\": \"‚úÖ\"\n",
        "                    if local_count == sf_count\n",
        "                    else \"‚ùå\"\n",
        "                    if isinstance(local_count, int)\n",
        "                    else \"‚ö†Ô∏è\",\n",
        "                }\n",
        "            )\n",
        "\n",
        "        conn.close()\n",
        "        df = pd.DataFrame(rows)\n",
        "        display(df)\n",
        "    else:\n",
        "        print(\"Snowflake connection not available - skipping comparison.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Incremental Update\n",
        "\n",
        "Demonstrate the day-2 workflow:\n",
        "\n",
        "1. Insert new rows into the **local** Polaris table\n",
        "2. Run `plf l2c update --force` to sync changes to S3 and refresh Snowflake\n",
        "3. Query Snowflake to confirm the new rows appear\n",
        "\n",
        "This proves that the L2C bridge works for ongoing development, not just\n",
        "one-shot migrations.\n",
        "\n",
        "> Requires both local cluster and `$PROJECT_HOME/bin/plf l2c setup` to have been run.  \n",
        "<!-- Skill action: l2c-update -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "if not has_l2c_state or not has_principal:\n",
        "    missing = []\n",
        "    if not has_principal:\n",
        "        missing.append(\"Polaris credentials (run: task setup:all WORK_DIR=<your-project>)\")\n",
        "    if not has_l2c_state:\n",
        "        missing.append(f\"L2C state (run: {project_root}/bin/plf l2c setup)\")\n",
        "    print(f\"\\u23ed\\ufe0f  Skipping -- missing: {', '.join(missing)}\")\n",
        "else:\n",
        "    import snowflake.connector\n",
        "\n",
        "    sf = state[\"snowflake\"]\n",
        "    database = sf[\"database\"]\n",
        "    schema = sf[\"schema\"]\n",
        "\n",
        "    # Pick the first registered table for the demo\n",
        "    demo_key = None\n",
        "    demo_info = None\n",
        "    for key, info in state.get(\"tables\", {}).items():\n",
        "        if info.get(\"register\", {}).get(\"status\") == \"done\":\n",
        "            demo_key = key\n",
        "            demo_info = info\n",
        "            break\n",
        "\n",
        "    if not demo_key:\n",
        "        print(f\"No registered tables found. Run: {project_root}/bin/plf l2c migrate\")\n",
        "    else:\n",
        "        ns = demo_info[\"namespace\"]\n",
        "        tbl = demo_info[\"table\"]\n",
        "        # Use the fully qualified Snowflake table name from register section\n",
        "        sf_table = demo_info.get(\"register\", {}).get(\"sf_table\", f\"{database}.{schema}.{demo_key.upper()}\")\n",
        "\n",
        "        # Get before count using DuckDB (same as Snowflake comparison section)\n",
        "        # Query the table for row count\n",
        "        local_count = get_table_count_via_duckdb(\n",
        "            polaris_url,\n",
        "            catalog_name,\n",
        "            client_id,\n",
        "            client_secret,\n",
        "            realm,\n",
        "            ns,\n",
        "            tbl,\n",
        "        )\n",
        "        \n",
        "        before_count = result[0] if result else 0\n",
        "        \n",
        "            \n",
        "        print(f\"Current row count in {ns}.{tbl}: {before_count}\")\n",
        "        print(\"Note: Incremental data insertion requires PyIceberg or direct SQL - skipping for demo\")\n",
        "        print(\"In a real scenario, you would insert data here, then run l2c update\")\n",
        "        \n",
        "        # Simulate the \"after\" count (in real demo, you'd insert actual data)\n",
        "        after_local = before_count  # No actual insertion in this demo\n",
        "\n",
        "        # Sync to cloud\n",
        "        plf = str(project_root / \"bin\" / \"plf\")\n",
        "        print(\"\\nRunning: plf l2c update --force --yes\")\n",
        "        subprocess.run([plf, \"l2c\", \"update\", \"--force\", \"--yes\"], cwd=project_root, check=True)\n",
        "\n",
        "        # Query Snowflake\n",
        "        conn = snowflake.connector.connect(\n",
        "            connection_name=cfg.get(\"SNOWFLAKE_DEFAULT_CONNECTION_NAME\", \"default\"),\n",
        "            role=sf[\"sa_role\"],\n",
        "            database=database,\n",
        "            schema=schema,\n",
        "        )\n",
        "        cur = conn.cursor()\n",
        "        cur.execute(f\"SELECT COUNT(*) FROM {sf_table}\")\n",
        "        sf_count = cur.fetchone()[0]\n",
        "        conn.close()\n",
        "\n",
        "        df = pd.DataFrame([{\n",
        "            \"table\": sf_table,\n",
        "            \"before\": before_count,\n",
        "            \"after_local\": after_local,\n",
        "            \"after_snowflake\": sf_count,\n",
        "            \"match\": \"Yes\" if after_local == sf_count else \"NO\",\n",
        "        }])\n",
        "        display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Reset and Reload (Demo Reset)\n",
        "\n",
        "Drop and recreate the local catalog to regenerate sample data, then clear\n",
        "the S3 bucket and Snowflake tables. This lets you re-run the full L2C\n",
        "migration demo **without** tearing down the k3d cluster.\n",
        "\n",
        "<!-- Skill action: l2c-reset -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "if not has_principal:\n",
        "    print(\"‚è≠Ô∏è  Skipping -- Polaris credentials not available.\")\n",
        "    print(\"   Run: task setup:all WORK_DIR=<your-project>\")\n",
        "else:\n",
        "    plf = str(project_root / \"bin\" / \"plf\")\n",
        "    print(f\"Using PLF binary: {plf}\")\n",
        "    print(f\"Working directory: {project_root}\")\n",
        "    print()\n",
        "\n",
        "    try:\n",
        "        print(\"=== Step 1: Drop and recreate catalog ===\")\n",
        "        result1 = subprocess.run([plf, \"catalog\", \"cleanup\", \"--yes\"], \n",
        "                                cwd=project_root, check=True, capture_output=True, text=True)\n",
        "        print(\"Catalog cleanup completed.\")\n",
        "        \n",
        "        result2 = subprocess.run([plf, \"catalog\", \"setup\"], \n",
        "                                cwd=project_root, check=True, capture_output=True, text=True)\n",
        "        print(\"Catalog recreated with sample data.\")\n",
        "        print()\n",
        "\n",
        "        if has_l2c_state:\n",
        "            print(\"=== Step 2: Clear S3 + Snowflake tables ===\")\n",
        "            result3 = subprocess.run([plf, \"l2c\", \"clear\", \"--yes\"], \n",
        "                                    cwd=project_root, check=True, capture_output=True, text=True)\n",
        "            print(\"S3 objects and Snowflake tables cleared.\")\n",
        "        else:\n",
        "            print(\"L2C not configured -- skipping S3/Snowflake clear.\")\n",
        "\n",
        "        print()\n",
        "        print(\"‚úÖ Ready to re-run L2C migration demo.\")\n",
        "        \n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Command failed: {e.cmd}\")\n",
        "        print(f\"Return code: {e.returncode}\")\n",
        "        if e.stdout:\n",
        "            print(f\"STDOUT: {e.stdout}\")\n",
        "        if e.stderr:\n",
        "            print(f\"STDERR: {e.stderr}\")\n",
        "        print()\n",
        "        print(\"üí° Troubleshooting:\")\n",
        "        print(f\"   - Check if PLF binary exists: ls -la {plf}\")\n",
        "        print(f\"   - Check working directory: ls -la {project_root}\")\n",
        "        print(\"   - Ensure cluster is running: task status\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
